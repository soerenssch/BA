{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18e73f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77ab3c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pd.read_excel(\"decoded_with_sources.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "856a5b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Combined data written to 'sampling_frame.db' with 258 rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sören Schlißke\\AppData\\Local\\Temp\\ipykernel_24900\\1538753107.py:25: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_df = pd.concat(collected_rows, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "collected_rows = []\n",
    "\n",
    "for _, row in master_df.iterrows():\n",
    "    slug = row['slug']\n",
    "    source = row['source_file']\n",
    "    if not os.path.exists(source):\n",
    "        print(f\"Source file not found: {source}\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        conn = sqlite3.connect(source)\n",
    "        query = \"SELECT * FROM sampled_collections WHERE slug_name = ?\"\n",
    "        result = pd.read_sql(query, conn, params=(slug,))\n",
    "        conn.close()\n",
    "        \n",
    "        if not result.empty:\n",
    "            collected_rows.append(result)\n",
    "        else:\n",
    "            print(f\"No data found for slug '{slug}' in {source}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading from {source} for slug {slug}: {e}\")\n",
    "\n",
    "# Combine all rows into one DataFrame\n",
    "if collected_rows:\n",
    "    combined_df = pd.concat(collected_rows, ignore_index=True)\n",
    "    \n",
    "    # Write to new SQLite DB\n",
    "    output_db = \"sampling_frame.db\"\n",
    "    conn_out = sqlite3.connect(output_db)\n",
    "    combined_df.to_sql(\"sampled_collections\", conn_out, index=False, if_exists=\"replace\")\n",
    "    conn_out.close()\n",
    "    \n",
    "    print(f\"✅ Combined data written to '{output_db}' with {len(combined_df)} rows.\")\n",
    "else:\n",
    "    print(\"⚠️ No data collected. Check slug names or DB file paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37f81126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_opensea_nfts_table(db_path=\"sampling_frame.db\", table_name=\"opensea_nfts\"):\n",
    "    \"\"\"\n",
    "    Creates a table with columns for the relevant fields from the OpenSea NFT JSON.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    # Create table if not exists\n",
    "    c.execute(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        \n",
    "        -- Linking field from 'sampled_collections' table\n",
    "        slug_name TEXT,\n",
    "        \n",
    "        -- Fields from the NFT JSON:\n",
    "        identifier TEXT,\n",
    "        contract TEXT,\n",
    "        token_standard TEXT,\n",
    "        name TEXT,\n",
    "        description TEXT,\n",
    "        image_url TEXT,\n",
    "        display_image_url TEXT,\n",
    "        display_animation_url TEXT,\n",
    "        metadata_url TEXT,\n",
    "        opensea_url TEXT,\n",
    "        updated_at TEXT,\n",
    "        is_disabled INTEGER,\n",
    "        is_nsfw INTEGER,\n",
    "        \n",
    "        fetched_at TEXT\n",
    "    )\n",
    "    \"\"\")\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def fetch_opensea_nfts(collection_slug, api_key, limit=10):\n",
    "    \"\"\"\n",
    "    Calls the OpenSea endpoint:\n",
    "      GET /api/v2/collection/{collection_slug}/nfts?limit={limit}\n",
    "    Returns the parsed JSON dict with:\n",
    "       { \"nfts\": [ { ... }, {...} ], \"next\": \"string\" }\n",
    "    If there's an error, returns an empty dict.\n",
    "    \"\"\"\n",
    "    base_url = \"https://api.opensea.io/api/v2/collection\"\n",
    "    url = f\"{base_url}/{collection_slug}/nfts\"\n",
    "    \n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"x-api-key\": api_key\n",
    "    }\n",
    "    params = {\n",
    "        \"limit\": limit\n",
    "    }\n",
    "    \n",
    "    resp = requests.get(url, headers=headers, params=params)\n",
    "    if resp.status_code == 200:\n",
    "        return resp.json()\n",
    "    else:\n",
    "        print(f\"[OpenSea NFT Fetch] Error {resp.status_code} for '{collection_slug}': {resp.text}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def store_opensea_nfts(db_path, table_name, slug_name, nfts_data):\n",
    "    \"\"\"\n",
    "    Inserts each NFT from nfts_data[\"nfts\"] into opensea_nfts, linking them\n",
    "    via 'slug_name'. We do NOT store the 'collection' field from the JSON,\n",
    "    but instead rely on 'slug_name' to identify the parent collection.\n",
    "    \"\"\"\n",
    "    nfts_list = nfts_data.get(\"nfts\", [])\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    insert_sql = f\"\"\"\n",
    "    INSERT INTO {table_name} (\n",
    "        slug_name,\n",
    "        identifier,\n",
    "        contract,\n",
    "        token_standard,\n",
    "        name,\n",
    "        description,\n",
    "        image_url,\n",
    "        display_image_url,\n",
    "        display_animation_url,\n",
    "        metadata_url,\n",
    "        opensea_url,\n",
    "        updated_at,\n",
    "        is_disabled,\n",
    "        is_nsfw,\n",
    "        fetched_at\n",
    "    )\n",
    "    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, datetime('now'))\n",
    "    \"\"\"\n",
    "    \n",
    "    for nft in nfts_list:\n",
    "        identifier = nft.get(\"identifier\", \"\")\n",
    "        contract = nft.get(\"contract\", \"\")\n",
    "        token_standard = nft.get(\"token_standard\", \"\")\n",
    "        name = nft.get(\"name\", \"\")\n",
    "        desc = nft.get(\"description\", \"\")\n",
    "        image_url = nft.get(\"image_url\", \"\")\n",
    "        display_image = nft.get(\"display_image_url\", \"\")\n",
    "        display_animation = nft.get(\"display_animation_url\", \"\")\n",
    "        metadata_url = nft.get(\"metadata_url\", \"\")\n",
    "        opensea_url = nft.get(\"opensea_url\", \"\")\n",
    "        updated_at = nft.get(\"updated_at\", \"\")\n",
    "        \n",
    "        # Convert booleans to int (1/0)\n",
    "        is_disabled = 1 if nft.get(\"is_disabled\", False) else 0\n",
    "        is_nsfw = 1 if nft.get(\"is_nsfw\", False) else 0\n",
    "        \n",
    "        c.execute(insert_sql, (\n",
    "            slug_name,\n",
    "            identifier,\n",
    "            contract,\n",
    "            token_standard,\n",
    "            name,\n",
    "            desc,\n",
    "            image_url,\n",
    "            display_image,\n",
    "            display_animation,\n",
    "            metadata_url,\n",
    "            opensea_url,\n",
    "            updated_at,\n",
    "            is_disabled,\n",
    "            is_nsfw\n",
    "        ))\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def fetch_opensea_nfts_for_collections(db_path=\"sampling_frame.db\", table_collections=\"sampled_collections\",\n",
    "                                       table_nfts=\"opensea_nfts\", api_key=\"YOUR_API_KEY\", limit=10):\n",
    "    \"\"\"\n",
    "    1) Ensure 'opensea_nfts' table exists.\n",
    "    2) Query 'sampled_collections' for marketplace='OpenSea', using 'slug_name'\n",
    "    3) For each, fetch up to {limit} items from OpenSea.\n",
    "    4) Store them in 'opensea_nfts'.\n",
    "    \"\"\"\n",
    "    # Create table if not exists\n",
    "    create_opensea_nfts_table(db_path, table_nfts)\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    # Query the relevant rows\n",
    "    c.execute(f\"\"\"\n",
    "    SELECT slug_name\n",
    "    FROM {table_collections}\n",
    "    WHERE marketplace='OpenSea'\n",
    "      AND slug_name IS NOT NULL\n",
    "    \"\"\")\n",
    "    rows = c.fetchall()\n",
    "    conn.close()\n",
    "    \n",
    "    for idx, (slug,) in enumerate(rows, start=1):\n",
    "        print(f\"[{idx}/{len(rows)}] Fetching up to {limit} NFTs for slug='{slug}'\")\n",
    "        nfts_data = fetch_opensea_nfts(slug, api_key, limit=limit)\n",
    "        store_opensea_nfts(db_path, table_nfts, slug, nfts_data)\n",
    "    \n",
    "    print(\"Done fetching NFT data for all OpenSea collections in sampling_frame.db.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9666d9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/76] Fetching up to 10 NFTs for slug='zora-posts-19156'\n",
      "[2/76] Fetching up to 10 NFTs for slug='titanshade'\n",
      "[3/76] Fetching up to 10 NFTs for slug='summer-vibe-10'\n",
      "[4/76] Fetching up to 10 NFTs for slug='stained-glasswindows'\n",
      "[5/76] Fetching up to 10 NFTs for slug='shib-chadmoney'\n",
      "[6/76] Fetching up to 10 NFTs for slug='me-physical-collectibles-10'\n",
      "[7/76] Fetching up to 10 NFTs for slug='lens-collect-profile-199929-publication-3'\n",
      "[8/76] Fetching up to 10 NFTs for slug='hell-angels'\n",
      "[9/76] Fetching up to 10 NFTs for slug='glow-panda-genesis'\n",
      "[10/76] Fetching up to 10 NFTs for slug='giants-g-nine-2024-emblem'\n",
      "[11/76] Fetching up to 10 NFTs for slug='dump-pepe-5'\n",
      "[12/76] Fetching up to 10 NFTs for slug='delfin-ohno-blast'\n",
      "[13/76] Fetching up to 10 NFTs for slug='chumpzgoded-season-one'\n",
      "[14/76] Fetching up to 10 NFTs for slug='bbbdf-e'\n",
      "[15/76] Fetching up to 10 NFTs for slug='zora-posts-21385'\n",
      "[16/76] Fetching up to 10 NFTs for slug='municipios-del-futuro-cluster-de-iot-de-la-comunid'\n",
      "[17/76] Fetching up to 10 NFTs for slug='mekaapes-blast'\n",
      "[18/76] Fetching up to 10 NFTs for slug='dude-lottery'\n",
      "[19/76] Fetching up to 10 NFTs for slug='city-s-art'\n",
      "[20/76] Fetching up to 10 NFTs for slug='unknowns-nft-1'\n",
      "[21/76] Fetching up to 10 NFTs for slug='sudocat-flow'\n",
      "[22/76] Fetching up to 10 NFTs for slug='rugpull-rehab-member'\n",
      "[23/76] Fetching up to 10 NFTs for slug='retro-rhapsody-1'\n",
      "[24/76] Fetching up to 10 NFTs for slug='open-ticketing-ecosystem-event-8355'\n",
      "[25/76] Fetching up to 10 NFTs for slug='oceanic-opulence-collection'\n",
      "[26/76] Fetching up to 10 NFTs for slug='hypio'\n",
      "[27/76] Fetching up to 10 NFTs for slug='galxe-oat-polygon-28'\n",
      "[28/76] Fetching up to 10 NFTs for slug='fire-348'\n",
      "[29/76] Fetching up to 10 NFTs for slug='dancing-girls-in-color'\n",
      "[30/76] Fetching up to 10 NFTs for slug='cybernetic-elegance'\n",
      "[31/76] Fetching up to 10 NFTs for slug='competitors-iso'\n",
      "[32/76] Fetching up to 10 NFTs for slug='cincinnati-physicians'\n",
      "[33/76] Fetching up to 10 NFTs for slug='zora-1531'\n",
      "[34/76] Fetching up to 10 NFTs for slug='user-gameplay-for-mine-rusher'\n",
      "[35/76] Fetching up to 10 NFTs for slug='testing-a-thing'\n",
      "[36/76] Fetching up to 10 NFTs for slug='sport-tuning-maintained'\n",
      "[37/76] Fetching up to 10 NFTs for slug='my-first-digital-drawings-2'\n",
      "[38/76] Fetching up to 10 NFTs for slug='genshin-impact-tcg'\n",
      "[39/76] Fetching up to 10 NFTs for slug='family-edition'\n",
      "[40/76] Fetching up to 10 NFTs for slug='cosmic-swell'\n",
      "[41/76] Fetching up to 10 NFTs for slug='xlfrw'\n",
      "[42/76] Fetching up to 10 NFTs for slug='thecryptomoms'\n",
      "[43/76] Fetching up to 10 NFTs for slug='nature-2596'\n",
      "[44/76] Fetching up to 10 NFTs for slug='mintak-2'\n",
      "[45/76] Fetching up to 10 NFTs for slug='legadotestingnfts-1'\n",
      "[46/76] Fetching up to 10 NFTs for slug='kellyvip-s-artworks'\n",
      "[47/76] Fetching up to 10 NFTs for slug='ibrahim-catabra-fan-club'\n",
      "[48/76] Fetching up to 10 NFTs for slug='how-to-pod'\n",
      "[49/76] Fetching up to 10 NFTs for slug='blueandpink'\n",
      "[50/76] Fetching up to 10 NFTs for slug='yousov-club'\n",
      "[51/76] Fetching up to 10 NFTs for slug='robert-deniro-1'\n",
      "[52/76] Fetching up to 10 NFTs for slug='oasis-73'\n",
      "[53/76] Fetching up to 10 NFTs for slug='loresbt-2'\n",
      "[54/76] Fetching up to 10 NFTs for slug='lens-collect-profile-126134-publication-890'\n",
      "[55/76] Fetching up to 10 NFTs for slug='goku-82'\n",
      "[56/76] Fetching up to 10 NFTs for slug='dragon-crypto-hero'\n",
      "[57/76] Fetching up to 10 NFTs for slug='lens-collect-profile-143257-publication-8280'\n",
      "[58/76] Fetching up to 10 NFTs for slug='lens-collect-profile-63990-publication-1168'\n",
      "[59/76] Fetching up to 10 NFTs for slug='summer-165'\n",
      "[60/76] Fetching up to 10 NFTs for slug='reliable-susda-1'\n",
      "[61/76] Fetching up to 10 NFTs for slug='hi-511'\n",
      "[62/76] Fetching up to 10 NFTs for slug='shepherd-20'\n",
      "[63/76] Fetching up to 10 NFTs for slug='alphaswap'\n",
      "[64/76] Fetching up to 10 NFTs for slug='personas-3'\n",
      "[65/76] Fetching up to 10 NFTs for slug='unidentified-contract-a5988893-4b18-46cf-926a-af97'\n",
      "[66/76] Fetching up to 10 NFTs for slug='aaaaaa-26'\n",
      "[67/76] Fetching up to 10 NFTs for slug='0x6f50933569ce5730b73720b5ee63d938126141f2'\n",
      "[68/76] Fetching up to 10 NFTs for slug='sei-of-the-dead-2'\n",
      "[69/76] Fetching up to 10 NFTs for slug='test-name-64'\n",
      "[70/76] Fetching up to 10 NFTs for slug='samuderakepri-adventure-pass'\n",
      "[71/76] Fetching up to 10 NFTs for slug='opensea-pro-8'\n",
      "[72/76] Fetching up to 10 NFTs for slug='farmine-lands'\n",
      "[73/76] Fetching up to 10 NFTs for slug='wrong-badge-rewards'\n",
      "[74/76] Fetching up to 10 NFTs for slug='flora-fauna-frames'\n",
      "[75/76] Fetching up to 10 NFTs for slug='alpha-mystic-collection'\n",
      "[76/76] Fetching up to 10 NFTs for slug='my-world-73'\n",
      "Done fetching NFT data for all OpenSea collections in sampling_frame.db.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    OPENSEA_API_KEY = \"32454cc75657485c8306c86fa37141ec\"\n",
    "    fetch_opensea_nfts_for_collections(\n",
    "        db_path=\"sampling_frame.db\",\n",
    "        table_collections=\"sampled_collections\",\n",
    "        table_nfts=\"opensea_nfts\",\n",
    "        api_key=OPENSEA_API_KEY,\n",
    "        limit=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1b774b",
   "metadata": {},
   "source": [
    "Rarible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "108496d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rarible_nfts_table(db_path=\"sample_1.db\", table_name=\"rarible_nfts\"):\n",
    "    \"\"\"\n",
    "    Creates a table 'rarible_nfts' with columns similar to 'opensea_nfts',\n",
    "    storing key fields from the Rarible item-level data plus a link back \n",
    "    to 'collection_id' in 'sampled_collections'.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    c.execute(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        -- references the original 'id:blockchain' format from 'sampled_collections'\n",
    "        collection_id TEXT,\n",
    "        \n",
    "        item_id TEXT,          -- item[\"id\"] e.g. \"ETHEREUM:0xb66a6...:123\"\n",
    "        blockchain TEXT,       -- item[\"blockchain\"]\n",
    "        contract TEXT,         -- item[\"contract\"]\n",
    "        token_id TEXT,         -- item[\"tokenId\"]\n",
    "        \n",
    "        name TEXT,             -- from item[\"meta\"][\"name\"]\n",
    "        description TEXT,      -- from item[\"meta\"][\"description\"]\n",
    "        image_url TEXT,        -- optional, if we parse item[\"meta\"][\"content\"] for an image\n",
    "        minted_at TEXT,        -- item[\"mintedAt\"]\n",
    "        last_updated TEXT,     -- item[\"lastUpdatedAt\"]\n",
    "        supply REAL,           -- item[\"supply\"] \n",
    "        owner_if_single TEXT,  -- item[\"ownerIfSingle\"] if we want\n",
    "        \n",
    "        project_url TEXT,      -- e.g. item[\"meta\"][\"externalUri\"] if present\n",
    "        created_at TEXT,       -- item[\"meta\"][\"createdAt\"]\n",
    "        \n",
    "        fetched_at TEXT\n",
    "    )\n",
    "    \"\"\")\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f878866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_collection_id_for_rarible(orig_id):\n",
    "    \"\"\"\n",
    "    Convert 'id:blockchain' => 'BLOCKCHAIN:0xid' for use in Rarible API.\n",
    "    e.g. '0x5948abcd:ethereum' => 'ETHEREUM:0x5948abcd'\n",
    "    \"\"\"\n",
    "    if \":\" not in orig_id:\n",
    "        return None\n",
    "    coll_id, chain = orig_id.split(\":\")\n",
    "    return f\"{chain.upper()}:{coll_id}\"\n",
    "\n",
    "def fetch_rarible_nfts(collection_param, api_key, size=10):\n",
    "    \"\"\"\n",
    "    Calls Rarible endpoint:\n",
    "      GET /v0.1/items/byCollection?collection={collection_param}&size={size}\n",
    "    e.g. collection_param = 'ETHEREUM:0xb66a603f...'\n",
    "    Returns a dict with keys 'continuation' and 'items' (list).\n",
    "    \"\"\"\n",
    "    url = \"https://api.rarible.org/v0.1/items/byCollection\"\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"X-API-KEY\": api_key\n",
    "    }\n",
    "    params = {\n",
    "        \"collection\": collection_param,\n",
    "        \"size\": size\n",
    "    }\n",
    "    resp = requests.get(url, headers=headers, params=params)\n",
    "    if resp.status_code == 200:\n",
    "        return resp.json()\n",
    "    else:\n",
    "        print(f\"[Rarible NFT Fetch] Error {resp.status_code} => {resp.text}\")\n",
    "        return {}\n",
    "\n",
    "def store_rarible_nfts(db_path, table_name, orig_id, items_list):\n",
    "    \"\"\"\n",
    "    Insert up to 10 items into 'rarible_nfts' table, storing relevant fields.\n",
    "    :param orig_id: The original 'id:blockchain' used in 'sampled_collections'\n",
    "    :param items_list: list of item dicts from the Rarible API\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    insert_sql = f\"\"\"\n",
    "    INSERT INTO {table_name} (\n",
    "        collection_id,\n",
    "        item_id,\n",
    "        blockchain,\n",
    "        contract,\n",
    "        token_id,\n",
    "        name,\n",
    "        description,\n",
    "        image_url,\n",
    "        minted_at,\n",
    "        last_updated,\n",
    "        supply,\n",
    "        owner_if_single,\n",
    "        project_url,\n",
    "        created_at,\n",
    "        fetched_at\n",
    "    )\n",
    "    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, datetime('now'))\n",
    "    \"\"\"\n",
    "    \n",
    "    for item in items_list:\n",
    "        item_id = item.get(\"id\", \"\")\n",
    "        blockchain = item.get(\"blockchain\", \"\")\n",
    "        contract = item.get(\"contract\", \"\")\n",
    "        token_id = str(item.get(\"tokenId\", \"\"))\n",
    "        supply = item.get(\"supply\", 0)\n",
    "        minted_at = item.get(\"mintedAt\", \"\")\n",
    "        last_updated = item.get(\"lastUpdatedAt\", \"\")\n",
    "        owner_if_single = item.get(\"ownerIfSingle\", \"\")\n",
    "        \n",
    "        meta = item.get(\"meta\", {})\n",
    "        name = meta.get(\"name\", \"\")\n",
    "        description = meta.get(\"description\", \"\")\n",
    "        created_at = meta.get(\"createdAt\", \"\")\n",
    "        project_url = meta.get(\"externalUri\", \"\")\n",
    "        \n",
    "        # Optional: parse meta[\"content\"] for image\n",
    "        # The example doesn't show a direct \"url\", so we may store 'N/A' or parse further\n",
    "        content_list = meta.get(\"content\", [])\n",
    "        image_url = \"N/A\"\n",
    "        for content_obj in content_list:\n",
    "            # if there's a recognized URL, parse it\n",
    "            # The example lacks a 'url' field, so we skip. \n",
    "            # If your real data has it, you might do:\n",
    "            # image_url = content_obj.get(\"url\", \"N/A\")\n",
    "            # break\n",
    "            pass\n",
    "        \n",
    "        c.execute(insert_sql, (\n",
    "            orig_id,      # collection_id (as it appears in 'sampled_collections')\n",
    "            item_id,\n",
    "            blockchain,\n",
    "            contract,\n",
    "            token_id,\n",
    "            name,\n",
    "            description,\n",
    "            image_url,\n",
    "            minted_at,\n",
    "            last_updated,\n",
    "            supply,\n",
    "            owner_if_single,\n",
    "            project_url,\n",
    "            created_at\n",
    "        ))\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def fetch_rarible_nfts_for_collections(db_path=\"sample_1.db\", table_collections=\"sampled_collections\",\n",
    "                                       table_nfts=\"rarible_nfts\", api_key=\"YOUR_RARIBLE_API_KEY\", size=10):\n",
    "    \"\"\"\n",
    "    1) Create the 'rarible_nfts' table if not exists.\n",
    "    2) Query 'sampled_collections' where marketplace='Rarible'.\n",
    "    3) Transform 'collection_id:blockchain' to 'BLOCKCHAIN:collection_id'.\n",
    "    4) Fetch items (size=10) from Rarible's endpoint.\n",
    "    5) Store them in 'rarible_nfts'.\n",
    "    \"\"\"\n",
    "    # 1) create table\n",
    "    create_rarible_nfts_table(db_path, table_nfts)\n",
    "    \n",
    "    # 2) query the relevant rows\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    c.execute(f\"\"\"\n",
    "    SELECT collection_id\n",
    "    FROM {table_collections}\n",
    "    WHERE marketplace='Rarible'\n",
    "      AND collection_id IS NOT NULL\n",
    "    \"\"\")\n",
    "    rows = c.fetchall()\n",
    "    conn.close()\n",
    "    \n",
    "    count = 0\n",
    "    for (orig_id,) in rows:\n",
    "        rarible_format = transform_collection_id_for_rarible(orig_id)\n",
    "        if not rarible_format:\n",
    "            print(f\"Skipping invalid format => {orig_id}\")\n",
    "            continue\n",
    "        \n",
    "        data = fetch_rarible_nfts(rarible_format, api_key, size=size)\n",
    "        items = data.get(\"items\", [])\n",
    "        \n",
    "        store_rarible_nfts(db_path, table_nfts, orig_id, items)\n",
    "        count += 1\n",
    "        print(f\"[{count}/{len(rows)}] Inserted up to {len(items)} items for {orig_id}\")\n",
    "    \n",
    "    print(\"Done fetching and storing Rarible NFTs in\", table_nfts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f880889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/112] Inserted up to 1 items for 0xb2ffa6075e0594cd62a2035381466cbf2f022274:zksync\n",
      "[2/112] Inserted up to 1 items for 0x8009c4e95f80cbf59127edafdeaa20eb7abf3563:base\n",
      "[3/112] Inserted up to 1 items for 0x482f1759dd48df2672c0b9a5fc6791b19f4fb7d3:etherlink\n",
      "[4/112] Inserted up to 10 items for 0x358f263b33c338719f4fa98ecc807a82228a902f:match\n",
      "[5/112] Inserted up to 1 items for 0xc219d80cac121b59c232041f72819d21e12e223ce8c543a7609a177b6fd16a0f:aptos\n",
      "[6/112] Inserted up to 10 items for 0x716ad1b6222046289c1664825cd9e4caf6253aec:match\n",
      "[7/112] Inserted up to 10 items for 0x1d3fb02ec926043c39f7b4b0bade786f36472471:base\n",
      "[8/112] Inserted up to 0 items for 0x55b1316d3065f4f88733d0aed9632a5ab8906ebd:base\n",
      "[9/112] Inserted up to 1 items for 0x77fef3c4c258da9d2a6ce739fa5436825ec28bb4:arbitrum\n",
      "[10/112] Inserted up to 4 items for 0x14021c550e81fccc20a3b37fceab9b5461dde99b:base\n",
      "[11/112] Inserted up to 0 items for 0xfe758872fe3a1c91b226dc280a4660850409e10a:astarzkevm\n",
      "[12/112] Inserted up to 10 items for 0x1cb901d907ff700e7a57a2fb7d23223eb4fb67b0:xai\n",
      "[13/112] Inserted up to 0 items for 0xa5aadd13e020e916a944ee981030c281d070e5ce:chiliz\n",
      "[14/112] Inserted up to 2 items for 0xe7cb6454128c78fa0baeb78e66f987ac434b2b41:base\n",
      "[15/112] Inserted up to 10 items for 0xb89b4ab2ba0c3b74ed4f247b41886cd116a5dc58:xai\n",
      "[16/112] Inserted up to 1 items for 0x78efade8669ab0a408750c067c06ab65c32204bf:ethereum\n",
      "[17/112] Inserted up to 1 items for 0x97683a99e531201de43f68c445a0325a426ebf7c:celo\n",
      "[18/112] Inserted up to 1 items for 0x6aeb5e6f18826f33debd54bfac55dae3bfa677dd:celo\n",
      "[19/112] Inserted up to 1 items for 0x9fcd53e9d106ceb6212f11cd6a20522283c3af53:polygon\n",
      "[20/112] Inserted up to 0 items for 0x436119dafa0e5160aa17e310da858ccfeb937c4e:ethereum\n",
      "[21/112] Inserted up to 0 items for 0x577cd319bc30fb0d8b59963aac3c317ae78b3c75:ethereum\n",
      "[22/112] Inserted up to 1 items for 0xdcbafc08b9af7c39ffb1c9e59e1f67ef8814033f:base\n",
      "[23/112] Inserted up to 2 items for 0xb9ed61f56f00b7cd1891a6b7ae84510b2be9b54b:ethereum\n",
      "[24/112] Inserted up to 7 items for 0xbc555711e648d5713e599016a878627838541495:base\n",
      "[25/112] Inserted up to 5 items for 0xc496e6fd76df4042cf2f1bf9721239fa30319c17:base\n",
      "[26/112] Inserted up to 10 items for 0x0c15b61197c8fca4322b8e6c4a744b597622a1dc:base\n",
      "[27/112] Inserted up to 1 items for 0x4e042f1d2f24ad0794a24a7a42d58b7c73f766af:kroma\n",
      "[28/112] Inserted up to 2 items for 0x7772c4bd4f3041c79597488346f7572ff9bc0402:shape\n",
      "[29/112] Inserted up to 10 items for 0xe6d1b14467afaf1dab6cab4c484b67da214937e8:immutablex\n",
      "[30/112] Inserted up to 10 items for 0x7e9804822c74a4d89ea975bb99201b34672650b2:base\n",
      "[31/112] Inserted up to 1 items for 0x5f5a2e9642d772a6d9b348621d0330ef0bc4222e:zksync\n",
      "[32/112] Inserted up to 1 items for 0x02dd7ab563709e0105b2f0a58ba51bc8b7573078:base\n",
      "[33/112] Inserted up to 10 items for 0x1ed3766693342343c198db04b2ae1390ac5495d9:alephzero\n",
      "[34/112] Inserted up to 1 items for 0x41ffb8407a23a1c1aa8b948677428e5049b850c1:base\n",
      "[35/112] Inserted up to 10 items for 0x6ba6f2207e343923ba692e5cae646fb0f566db8d:ethereum\n",
      "[36/112] Inserted up to 2 items for 0x2707206cda1a7a07936001759fe753437e6a7607:ethereum\n",
      "[37/112] Inserted up to 1 items for 0x1f4d1192af2a92f288f380e570682e5f82feb667b396c5d5ae517e6b6c0ea465:aptos\n",
      "[38/112] Inserted up to 1 items for 0x594824a3d6e5777b3c7cc202ad1050435aac7698:ethereum\n",
      "[39/112] Inserted up to 8 items for 0x231470ba51c0316c0d1dc2432d495825a960de91:ethereum\n",
      "[40/112] Inserted up to 1 items for 0xbd88289d94f28cae7a5ea1136be8f78b863bb1cd:base\n",
      "[41/112] Inserted up to 1 items for 0x68de02dab89a266858d74e2ca6707693fff022f16d9d9020f52e08271acade84:aptos\n",
      "[42/112] Inserted up to 1 items for 0xd75185595d3f14d8ebc030190af33858917174600a5a7784ee9b51043f40fae1:aptos\n",
      "[43/112] Inserted up to 1 items for 0xaf54122faf8c71e662f734aad49aa8173e7de856:arbitrum\n",
      "[44/112] Inserted up to 10 items for 0x67f89da33c80cd15e3f692afa27e9641868a072a:alephzero\n",
      "[45/112] Inserted up to 4 items for 0xbefc6b1ac05ecff75f8f1866d801e990d01dc0bd:telos\n",
      "[46/112] Inserted up to 10 items for 0x23b5ba0f50664ce3a48ded214e773eb430818e65:celo\n",
      "[47/112] Inserted up to 8 items for 0x8d7ab3643263b1d676c1ac63bc38a99d4e720ee7:ethereum\n",
      "[48/112] Inserted up to 2 items for 0xf3e599db00a9a41785b619cbc87c86019349dda5:lisk\n",
      "[49/112] Inserted up to 10 items for 0x00b8f905c1ece49f78676733073e308fde4da737:base\n",
      "[50/112] Inserted up to 1 items for 0x96a7aff0a3a8657d7b515e01c1e94d1f55d37fe12868930a2d33ab690ff1bcbb:aptos\n",
      "[51/112] Inserted up to 5 items for 0x64378902d6afb97eb222d3a02cc1c146acfb1c0e:base\n",
      "[52/112] Inserted up to 4 items for 0x2583c114667f57038f5b2ac0b9f23f06f7f1908a:ethereum\n",
      "[53/112] Inserted up to 1 items for 0xc07910d1ff47b5b88a346ea1645429307e402f67:base\n",
      "[54/112] Inserted up to 10 items for 0x47dc9331dab033e4b8f3f3cf46f0a91bba80cb7f:base\n",
      "[55/112] Inserted up to 1 items for 0x1a5de3bac5917a806cdaaac29a4391e517794ce5:base\n",
      "[56/112] Inserted up to 1 items for 0x23b78604c09d32f6dba130ad4bab0c6a0c272693:base\n",
      "[57/112] Inserted up to 2 items for 0x471c250781cd7a64ab37f227e9ceee956cc9ca7a:base\n",
      "[58/112] Inserted up to 1 items for 0x5af0171ae867290357f2aef703a163f10f4b74b144534941457d3f5e217cf02c:aptos\n",
      "[59/112] Inserted up to 1 items for 0x84f2789475754572311d65173ade3c24e643ba29:base\n",
      "[60/112] Inserted up to 10 items for 0x735d8a60455acf37a617e241163e2d593b9eb439:base\n",
      "[61/112] Inserted up to 1 items for 0x57e104e68bbca6565486848349994d754ad61866:base\n",
      "[62/112] Inserted up to 1 items for 0x2387084d7ef28b12babc80dca429b267967a33c2:base\n",
      "[63/112] Inserted up to 1 items for 0x55d048ad2aad7a3218f8d87ed6f7d483f86fa0de565c2da39f1a8b654f54b5a1:aptos\n",
      "[64/112] Inserted up to 5 items for 0x469f7afa52541afb334373368bf6d8dce891f219:base\n",
      "[65/112] Inserted up to 1 items for 0xf1f7c338da9e92b95489187bc145d31e781063d6:base\n",
      "[66/112] Inserted up to 10 items for 0xbdf2cefc92dde9a9e276d7c710fdaa0f28663af1:base\n",
      "[67/112] Inserted up to 10 items for 0x8291bb25f8e1fc88a94294df79ca98e9de81f5d9:base\n",
      "[68/112] Inserted up to 2 items for 0xac90285fa79b6716f40bc4ddb661d0748edc8028:polygon\n",
      "[69/112] Inserted up to 1 items for 0x2379380abee993971f371b58c368b429c7394e4f:celo\n",
      "[70/112] Inserted up to 1 items for 0xa1aac0acda153d1a0da6afadb4b044597010ae42:base\n",
      "[71/112] Inserted up to 10 items for 0x8aa932f0e1daa959bdcedc7b696a96908dbf091e:base\n",
      "[72/112] Inserted up to 10 items for 0x38aebd5ad1b71374cc59f5f1271c29e3cfb62caa:rari\n",
      "[73/112] Inserted up to 5 items for 0xb7be41cfb1eb2f15c16e144ba2e90eb8828749ab:kroma\n",
      "[74/112] Inserted up to 0 items for 0x4075e90b77c57f32906febc46f4e5e5a0640a34b:moonbeam\n",
      "[75/112] Inserted up to 0 items for 0x866287c6d364ad0c281e3f6d70c896cdf5477a75:moonbeam\n",
      "[76/112] Inserted up to 2 items for 0xd38b9e8dc82b5ffc0fbf1350bde0361f4bc755e4:palm\n",
      "[77/112] Inserted up to 10 items for 0x1195cf65f83b3a5768f3c496d3a05ad6412c64b7:etherlink\n",
      "[78/112] Inserted up to 0 items for 0xa990c7046b03ab3cd3a0b9bfb507e06a5649a2e2:shape\n",
      "[79/112] Inserted up to 1 items for 0xa46205f220df8ee5bfe11b2b672e729893f59f68:telos\n",
      "[80/112] Inserted up to 1 items for 0xa10d31341ddcc7d848626fe405bc208d5f8b64a4:arbitrum\n",
      "[81/112] Inserted up to 0 items for 0xeeec1bda08cc5a6a26d43406c08d0d35bd54d830:abstract\n",
      "[82/112] Inserted up to 0 items for 0x7b25869dd5405b82b0debf48642eaca4e2a61a20:abstract\n",
      "[83/112] Inserted up to 10 items for 0x552b2bddc5adca46b962439da9610699e8b4f78a:immutablex\n",
      "[84/112] Inserted up to 4 items for 0x5194161b237be56026edc9230a0a0ad859746bb5:polygon\n",
      "[85/112] Inserted up to 10 items for 0x337ae55935cac7358c724b54eece2176af53d3f4:xai\n",
      "[86/112] Inserted up to 8 items for 0x06bfab6bc13c9627ca2b7f88996be979808ad5b4:xai\n",
      "[87/112] Inserted up to 10 items for 0xa059d2063452684546c3064ade4f895456930507:xai\n",
      "[88/112] Inserted up to 10 items for 0xe775ae6866d57e2ccd402d3173a2fd17fdf9b2ba:xai\n",
      "[89/112] Inserted up to 10 items for 0x8f89d432d50343b1bfb398cf159f0fd34d47de58:xai\n",
      "[90/112] Inserted up to 2 items for 0x6bb41a1702165914237af14e3a69cbaabe7a7b20:rari\n",
      "[91/112] Inserted up to 3 items for 0x190fe9f1e412234c47ea7dc510c1f20b4e5e590b:palm\n",
      "[92/112] Inserted up to 0 items for 0xd234a6313f0518190b316186162e3683e5da155d:chiliz\n",
      "[93/112] Inserted up to 0 items for 0x26b1df512af1ba17f04670d490224a8f6bd8d4ff:chiliz\n",
      "[94/112] Inserted up to 0 items for 0x1ca201971d97c62a7043516e654e375ec752442b:saakuru\n",
      "[95/112] Inserted up to 0 items for 0xa8b3e3eade7686158bad5a5d066ce6fed27cb802:base\n",
      "[96/112] Inserted up to 10 items for 0xd1731128ab1fd2ee55fc09829d3ff96fcbfa6ef7:base\n",
      "[97/112] Inserted up to 0 items for 0xa891b9211ce3b2a5bfc6f8bdbd837cda0641379a:ethereum\n",
      "[98/112] Inserted up to 10 items for 0x193ba2c46519459ad536ea580978faffef575ba7:polygon\n",
      "[99/112] Inserted up to 1 items for 0x47339e603a5091c262f29a4a393d39762fe0c7de:base\n",
      "[100/112] Inserted up to 1 items for 0x5d4c782843b8fe2bdc3f9112230666093d93d103:base\n",
      "[101/112] Inserted up to 3 items for 0x93eca311f3a6f80df3a21a85fcfceeecf97f644b:polygon\n",
      "[102/112] Inserted up to 10 items for 0xafc459d37c37d68901da0cca0205cf54085fc9e9:polygon\n",
      "[103/112] Inserted up to 1 items for 0x967fd257f8978b3338df1879baf074837cf2d853:ethereum\n",
      "[104/112] Inserted up to 1 items for 0xc1782028a6cb43f57152724aaff8a93f11e73802:celo\n",
      "[105/112] Inserted up to 0 items for 0xcca474855e1c57704abb5bc5e7fa7fa34b71e922:arbitrum\n",
      "[106/112] Inserted up to 1 items for 0xbedc1d55d03d7c338c40db9232989ea8321b4df32faedd31e14f5abea45bb386:aptos\n",
      "[107/112] Inserted up to 1 items for 0xa3330ca4cd7ce3db9f6ca691b94f49c503dedca300ec6599cafa7e577ac946fc:aptos\n",
      "[108/112] Inserted up to 1 items for 0x6ae2536b3e65b0e5109d22862eed582417d8cc4c7356eeb10b779dc44ae939f7:aptos\n",
      "[109/112] Inserted up to 9 items for 0xc53b1b9e4765132b1bad688cec0e2f4b64c0131c:polygon\n",
      "[110/112] Inserted up to 10 items for 0x10ff1777808e4ae4f93af45d88e4a12599153da4:polygon\n",
      "[111/112] Inserted up to 1 items for 0xbc8678cba2008b00d67d098b640a575782864c6c:ethereum\n",
      "[112/112] Inserted up to 4 items for 0x09bd3e86d019c3520f51305e2cc08d93d4f66546:ethereum\n",
      "Done fetching and storing Rarible NFTs in rarible_nfts\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    RARIBLE_API_KEY = \"1c4706e0-6fe2-4181-bb4d-70322cb866b0\"\n",
    "    fetch_rarible_nfts_for_collections(\n",
    "        db_path=\"sampling_frame.db\",\n",
    "        table_collections=\"sampled_collections\",\n",
    "        table_nfts=\"rarible_nfts\",\n",
    "        api_key=RARIBLE_API_KEY,\n",
    "        size=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904b5d21",
   "metadata": {},
   "source": [
    "MagicEden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc031682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_magiceden_nfts_table(db_path=\"sample_1.db\", table_name=\"magiceden_nfts\"):\n",
    "    \"\"\"\n",
    "    Creates a table 'magiceden_nfts' with columns for relevant fields from\n",
    "    the Magic Eden listing JSON, plus a link back to 'sampled_collections' via slug_name.\n",
    "    \n",
    "    We'll have 27 columns total:\n",
    "      1) id (PK)\n",
    "      2) collection_slug\n",
    "      ...\n",
    "      26) token_properties_json\n",
    "      27) fetched_at\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    # Below we define 27 columns total\n",
    "    c.execute(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        \n",
    "        collection_slug TEXT,\n",
    "\n",
    "        pdaAddress TEXT,\n",
    "        auctionHouse TEXT,\n",
    "        tokenAddress TEXT,\n",
    "        tokenMint TEXT,\n",
    "        seller TEXT,\n",
    "        sellerReferral TEXT,\n",
    "        tokenSize REAL,\n",
    "        price REAL,\n",
    "        expiry REAL,\n",
    "        \n",
    "        rarity_json TEXT,\n",
    "        extra_json TEXT,\n",
    "        listingSource TEXT,\n",
    "        \n",
    "        token_mintAddress TEXT,\n",
    "        token_owner TEXT,\n",
    "        token_supply REAL,\n",
    "        token_collection TEXT,\n",
    "        token_name TEXT,\n",
    "        token_updateAuthority TEXT,\n",
    "        token_primarySaleHappened INTEGER,\n",
    "        token_sellerFeeBasisPoints REAL,\n",
    "        token_image TEXT,\n",
    "        token_animationUrl TEXT,\n",
    "        token_externalUrl TEXT,\n",
    "        \n",
    "        token_attributes_json TEXT,\n",
    "        token_properties_json TEXT,\n",
    "        \n",
    "        fetched_at TEXT\n",
    "    )\n",
    "    \"\"\")\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def fetch_magiceden_listings(collection_slug, limit=10):\n",
    "    \"\"\"\n",
    "    GET https://api-mainnet.magiceden.dev/v2/collections/{collection_slug}/listings?limit={limit}\n",
    "    Returns a list of dicts if successful, else empty list.\n",
    "    \"\"\"\n",
    "    base_url = \"https://api-mainnet.magiceden.dev/v2/collections\"\n",
    "    url = f\"{base_url}/{collection_slug}/listings\"\n",
    "    params = {\"limit\": limit}\n",
    "    headers = {\"accept\": \"application/json\"}\n",
    "    \n",
    "    resp = requests.get(url, headers=headers, params=params)\n",
    "    if resp.status_code == 200:\n",
    "        return resp.json()  # Should be a list of listing dicts\n",
    "    else:\n",
    "        print(f\"[MagicEden NFT Fetch] Error {resp.status_code} for slug='{collection_slug}': {resp.text}\")\n",
    "        return []\n",
    "\n",
    "def store_magiceden_nfts(db_path, table_name, slug_name, listings):\n",
    "    \"\"\"\n",
    "    Insert each listing item into 'magiceden_nfts'.\n",
    "    We must provide EXACTLY 26 placeholders for these columns (excluding 'id' + including 'fetched_at' as a datetime call).\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    # 27 columns total => 'id' is autoincrement, so we have 26 placeholders\n",
    "    insert_sql = f\"\"\"\n",
    "    INSERT INTO {table_name} (\n",
    "        collection_slug,\n",
    "        \n",
    "        pdaAddress,\n",
    "        auctionHouse,\n",
    "        tokenAddress,\n",
    "        tokenMint,\n",
    "        seller,\n",
    "        sellerReferral,\n",
    "        tokenSize,\n",
    "        price,\n",
    "        expiry,\n",
    "        \n",
    "        rarity_json,\n",
    "        extra_json,\n",
    "        listingSource,\n",
    "        \n",
    "        token_mintAddress,\n",
    "        token_owner,\n",
    "        token_supply,\n",
    "        token_collection,\n",
    "        token_name,\n",
    "        token_updateAuthority,\n",
    "        token_primarySaleHappened,\n",
    "        token_sellerFeeBasisPoints,\n",
    "        token_image,\n",
    "        token_animationUrl,\n",
    "        token_externalUrl,\n",
    "        \n",
    "        token_attributes_json,\n",
    "        token_properties_json,\n",
    "        \n",
    "        fetched_at\n",
    "    )\n",
    "    VALUES (\n",
    "        ?,  -- collection_slug\n",
    "        ?,  -- pdaAddress\n",
    "        ?,  -- auctionHouse\n",
    "        ?,  -- tokenAddress\n",
    "        ?,  -- tokenMint\n",
    "        ?,  -- seller\n",
    "        ?,  -- sellerReferral\n",
    "        ?,  -- tokenSize\n",
    "        ?,  -- price\n",
    "        ?,  -- expiry\n",
    "        ?,  -- rarity_json\n",
    "        ?,  -- extra_json\n",
    "        ?,  -- listingSource\n",
    "        ?,  -- token_mintAddress\n",
    "        ?,  -- token_owner\n",
    "        ?,  -- token_supply\n",
    "        ?,  -- token_collection\n",
    "        ?,  -- token_name\n",
    "        ?,  -- token_updateAuthority\n",
    "        ?,  -- token_primarySaleHappened\n",
    "        ?,  -- token_sellerFeeBasisPoints\n",
    "        ?,  -- token_image\n",
    "        ?,  -- token_animationUrl\n",
    "        ?,  -- token_externalUrl\n",
    "        ?,  -- token_attributes_json\n",
    "        ?,  -- token_properties_json\n",
    "        datetime('now')  -- fetched_at\n",
    "    )\n",
    "    \"\"\"\n",
    "    # Notice we have exactly 26 placeholders (the last column uses datetime('now')).\n",
    "\n",
    "    for listing in listings:\n",
    "        pdaAddress = listing.get(\"pdaAddress\", \"\")\n",
    "        auctionHouse = listing.get(\"auctionHouse\", \"\")\n",
    "        tokenAddress = listing.get(\"tokenAddress\", \"\")\n",
    "        tokenMint = listing.get(\"tokenMint\", \"\")\n",
    "        seller = listing.get(\"seller\", \"\")\n",
    "        sellerReferral = listing.get(\"sellerReferral\", \"\")\n",
    "        tokenSize = listing.get(\"tokenSize\", 0)\n",
    "        price = listing.get(\"price\", 0)\n",
    "        expiry = listing.get(\"expiry\", 0)\n",
    "        \n",
    "        # Convert 'rarity' and 'extra' to JSON strings\n",
    "        rarity_json = json.dumps(listing.get(\"rarity\", {}))\n",
    "        extra_json = json.dumps(listing.get(\"extra\", {}))\n",
    "        \n",
    "        listingSource = listing.get(\"listingSource\", \"\")\n",
    "        \n",
    "        token_obj = listing.get(\"token\", {})\n",
    "        token_mintAddress = token_obj.get(\"mintAddress\", \"\")\n",
    "        token_owner = token_obj.get(\"owner\", \"\")\n",
    "        token_supply = token_obj.get(\"supply\", 0)\n",
    "        token_collection = token_obj.get(\"collection\", \"\")\n",
    "        token_name = token_obj.get(\"name\", \"\")\n",
    "        token_updateAuthority = token_obj.get(\"updateAuthority\", \"\")\n",
    "        \n",
    "        primarySaleHappened = 1 if token_obj.get(\"primarySaleHappened\", False) else 0\n",
    "        sellerFeeBasisPoints = token_obj.get(\"sellerFeeBasisPoints\", 0)\n",
    "        token_image = token_obj.get(\"image\", \"\")\n",
    "        token_animationUrl = token_obj.get(\"animationUrl\", \"\")\n",
    "        token_externalUrl = token_obj.get(\"externalUrl\", \"\")\n",
    "        \n",
    "        attributes_json = json.dumps(token_obj.get(\"attributes\", []))\n",
    "        properties_json = json.dumps(token_obj.get(\"properties\", {}))\n",
    "        \n",
    "        c.execute(insert_sql, (\n",
    "            slug_name,\n",
    "            pdaAddress,\n",
    "            auctionHouse,\n",
    "            tokenAddress,\n",
    "            tokenMint,\n",
    "            seller,\n",
    "            sellerReferral,\n",
    "            tokenSize,\n",
    "            price,\n",
    "            expiry,\n",
    "            rarity_json,\n",
    "            extra_json,\n",
    "            listingSource,\n",
    "            token_mintAddress,\n",
    "            token_owner,\n",
    "            token_supply,\n",
    "            token_collection,\n",
    "            token_name,\n",
    "            token_updateAuthority,\n",
    "            primarySaleHappened,\n",
    "            sellerFeeBasisPoints,\n",
    "            token_image,\n",
    "            token_animationUrl,\n",
    "            token_externalUrl,\n",
    "            attributes_json,\n",
    "            properties_json\n",
    "            # fetched_at => datetime('now') in SQL\n",
    "        ))\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def fetch_magiceden_nfts_for_collections(db_path=\"sample_1.db\",\n",
    "                                         table_collections=\"sampled_collections\",\n",
    "                                         table_nfts=\"magiceden_nfts\",\n",
    "                                         limit=10):\n",
    "    \"\"\"\n",
    "    Creates table if necessary, then for each row with marketplace='MagicEden',\n",
    "    uses 'slug_name' to fetch up to {limit} items from Magic Eden,\n",
    "    storing them in magiceden_nfts.\n",
    "    \"\"\"\n",
    "    create_magiceden_nfts_table(db_path, table_nfts)\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    c.execute(f\"\"\"\n",
    "    SELECT slug_name\n",
    "    FROM {table_collections}\n",
    "    WHERE marketplace='MagicEden'\n",
    "      AND slug_name IS NOT NULL\n",
    "    \"\"\")\n",
    "    rows = c.fetchall()\n",
    "    conn.close()\n",
    "    \n",
    "    count = 0\n",
    "    for (slug_name,) in rows:\n",
    "        data = fetch_magiceden_listings(slug_name, limit=limit)\n",
    "        store_magiceden_nfts(db_path, table_nfts, slug_name, data)\n",
    "        count += 1\n",
    "        print(f\"[{count}/{len(rows)}] Inserted up to {len(data)} items for MagicEden => {slug_name}\")\n",
    "    \n",
    "    print(f\"Done fetching/storing MagicEden listings in {table_nfts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a7401a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/50] Inserted up to 10 items for MagicEden => world_cup_frog\n",
      "[2/50] Inserted up to 10 items for MagicEden => synergy_land\n",
      "[3/50] Inserted up to 10 items for MagicEden => solshades_vouchers\n",
      "[4/50] Inserted up to 10 items for MagicEden => salt_bandits\n",
      "[5/50] Inserted up to 0 items for MagicEden => p2p\n",
      "[6/50] Inserted up to 10 items for MagicEden => monkelines\n",
      "[7/50] Inserted up to 0 items for MagicEden => justcoffee\n",
      "[8/50] Inserted up to 10 items for MagicEden => bunny_state\n",
      "[9/50] Inserted up to 0 items for MagicEden => blue_gold_collection\n",
      "[10/50] Inserted up to 10 items for MagicEden => bit_monkey_club\n",
      "[11/50] Inserted up to 10 items for MagicEden => yetiz\n",
      "[12/50] Inserted up to 7 items for MagicEden => swirlogy\n",
      "[13/50] Inserted up to 10 items for MagicEden => islesofmeta\n",
      "[14/50] Inserted up to 5 items for MagicEden => dramwolf\n",
      "[15/50] Inserted up to 10 items for MagicEden => abc_marvel\n",
      "[16/50] Inserted up to 7 items for MagicEden => spotted_lanternflies\n",
      "[17/50] Inserted up to 10 items for MagicEden => primordials\n",
      "[18/50] Inserted up to 10 items for MagicEden => modern_drummer\n",
      "[19/50] Inserted up to 10 items for MagicEden => kyoudai_spirits\n",
      "[20/50] Inserted up to 2 items for MagicEden => kpcfs1\n",
      "[21/50] Inserted up to 2 items for MagicEden => chorkean_\n",
      "[22/50] Inserted up to 10 items for MagicEden => De_Casinos\n",
      "[23/50] Inserted up to 10 items for MagicEden => pawspaws\n",
      "[24/50] Inserted up to 10 items for MagicEden => jnkflip\n",
      "[25/50] Inserted up to 10 items for MagicEden => goopywrldheroes\n",
      "[26/50] Inserted up to 10 items for MagicEden => frostys_friends\n",
      "[27/50] Inserted up to 10 items for MagicEden => drip_genopets_s2\n",
      "[28/50] Inserted up to 10 items for MagicEden => cool_penguin_squad\n",
      "[29/50] Inserted up to 1 items for MagicEden => art_and_soul\n",
      "[30/50] Inserted up to 3 items for MagicEden => zcoinsol\n",
      "[31/50] Inserted up to 10 items for MagicEden => solana_mecha_apes\n",
      "[32/50] Inserted up to 0 items for MagicEden => pixelated_apes_dao\n",
      "[33/50] Inserted up to 10 items for MagicEden => p2_farmers_gs\n",
      "[34/50] Inserted up to 6 items for MagicEden => ones_\n",
      "[35/50] Inserted up to 10 items for MagicEden => oak_paradise\n",
      "[36/50] Inserted up to 10 items for MagicEden => neon_clouds_collective\n",
      "[37/50] Inserted up to 1 items for MagicEden => mrclpass\n",
      "[38/50] Inserted up to 10 items for MagicEden => moveman\n",
      "[39/50] Inserted up to 10 items for MagicEden => heavenland\n",
      "[40/50] Inserted up to 10 items for MagicEden => coast2coast\n",
      "[41/50] Inserted up to 10 items for MagicEden => titan_whales\n",
      "[42/50] Inserted up to 10 items for MagicEden => the_frontier\n",
      "[43/50] Inserted up to 10 items for MagicEden => skatex_events\n",
      "[44/50] Inserted up to 5 items for MagicEden => metavillagemansions\n",
      "[45/50] Inserted up to 7 items for MagicEden => dflip\n",
      "[46/50] Inserted up to 1 items for MagicEden => cyberspies\n",
      "[47/50] Inserted up to 0 items for MagicEden => cufives_nfnc\n",
      "[48/50] Inserted up to 10 items for MagicEden => SAW_Games_Pass\n",
      "[49/50] Inserted up to 10 items for MagicEden => stellaravatars\n",
      "[50/50] Inserted up to 1 items for MagicEden => tropic_tickets\n",
      "Done fetching/storing MagicEden listings in magiceden_nfts\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    fetch_magiceden_nfts_for_collections(\n",
    "        db_path=\"sampling_frame.db\",\n",
    "        table_collections=\"sampled_collections\",\n",
    "        table_nfts=\"magiceden_nfts\",\n",
    "        limit=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a00cf30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_columns_if_not_exists(db_path=\"sampling_frame.db\", table_name=\"sampled_collections\"):\n",
    "    \"\"\"\n",
    "    Adds the column 'total_supply' to the 'sampled_collections' table\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    cols_to_add = [\n",
    "        (\"total_supply\", \"REAL\")\n",
    "    ]\n",
    "    \n",
    "    for col, col_type in cols_to_add:\n",
    "        alter_stmt = f\"ALTER TABLE {table_name} ADD COLUMN {col} {col_type}\"\n",
    "        try:\n",
    "            c.execute(alter_stmt)\n",
    "            print(f\"Added column '{col}' to {table_name}\")\n",
    "        except sqlite3.OperationalError as e:\n",
    "            # Likely column already exists\n",
    "            pass\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c430514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_atomic_collection_stats(collection_name):\n",
    "    \"\"\"\n",
    "    Calls /atomicassets/v1/collections/{collection_name}/stats\"\n",
    "    \"\"\"\n",
    "    url = f\"https://wax.api.atomicassets.io/atomicassets/v1/collections/{collection_name}/stats\"\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code == 200:\n",
    "        return resp.json()\n",
    "    else:\n",
    "        print(f\"[Atomic Stats] Error {resp.status_code} for collection='{collection_name}': {resp.text}\")\n",
    "        return {}\n",
    "\n",
    "def update_atomic_collection_total_supply(db_path=\"sample_1.db\",\n",
    "                                          table_name=\"sampled_collections\"):\n",
    "    \"\"\"\n",
    "    1) Query 'sampled_collections' where marketplace='Atomic',\n",
    "       using 'slug_name' as the collection_name.\n",
    "    2) For each row, call fetch_atomic_collection_stats(...),\n",
    "       parse data[\"data\"][\"assets\"], and store it in 'total_supply'.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    c.execute(f\"\"\"\n",
    "    SELECT slug_name\n",
    "    FROM {table_name}\n",
    "    WHERE marketplace='Atomic'\n",
    "      AND slug_name IS NOT NULL\n",
    "    \"\"\")\n",
    "    rows = c.fetchall()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"Found {len(rows)} 'Atomic' collections to update total_supply.\")\n",
    "    \n",
    "    count = 0\n",
    "    for (collection_name,) in rows:\n",
    "        stats_data = fetch_atomic_collection_stats(collection_name)\n",
    "        # stats_data => { \"success\": bool, \"data\": {...}, ... }\n",
    "        if not stats_data.get(\"success\", False):\n",
    "            print(f\"Skipping {collection_name}, 'success' is False or missing.\")\n",
    "            continue\n",
    "        \n",
    "        # If \"assets\" is None or missing, default to \"0\"\n",
    "        assets_str = stats_data.get(\"data\", {}).get(\"assets\")\n",
    "        if assets_str is None:\n",
    "            assets_str = \"0\"\n",
    "        \n",
    "        try:\n",
    "            assets_count = int(assets_str)\n",
    "        except ValueError:\n",
    "            assets_count = 0  # or handle differently if needed\n",
    "        \n",
    "        conn2 = sqlite3.connect(db_path)\n",
    "        c2 = conn2.cursor()\n",
    "        update_stmt = f\"\"\"\n",
    "        UPDATE {table_name}\n",
    "        SET total_supply = ?\n",
    "        WHERE marketplace='Atomic'\n",
    "          AND slug_name=?\n",
    "        \"\"\"\n",
    "        c2.execute(update_stmt, (assets_count, collection_name))\n",
    "        conn2.commit()\n",
    "        conn2.close()\n",
    "        \n",
    "        count += 1\n",
    "        print(f\"[{count}/{len(rows)}] Updated total_supply={assets_count} for {collection_name}\")\n",
    "    \n",
    "    print(\"Done updating total_supply for Atomic collections.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ff588f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added column 'total_supply' to sampled_collections\n",
      "Found 20 'Atomic' collections to update total_supply.\n",
      "[1/20] Updated total_supply=30 for dimebagswaxd\n",
      "[2/20] Updated total_supply=5 for alienwor1dsz\n",
      "[3/20] Updated total_supply=6 for watercolorzz\n",
      "[4/20] Updated total_supply=1 for adidasshoes3\n",
      "[5/20] Updated total_supply=15246 for varialandsio\n",
      "[6/20] Updated total_supply=22 for waxwalletsto\n",
      "[7/20] Updated total_supply=3 for ycjhajx5bcol\n",
      "[8/20] Updated total_supply=2 for vikez1modern\n",
      "[9/20] Updated total_supply=10 for nftartdesing\n",
      "[10/20] Updated total_supply=5 for bestpepeever\n",
      "[11/20] Updated total_supply=1 for agoratest123\n",
      "[12/20] Updated total_supply=22 for oddsandendss\n",
      "[13/20] Updated total_supply=24 for agruneforyou\n",
      "[14/20] Updated total_supply=5 for 1a2b3c4d5e11\n",
      "[15/20] Updated total_supply=20 for targetedby11\n",
      "[16/20] Updated total_supply=0 for ondjlfgjbuzp\n",
      "[17/20] Updated total_supply=10 for dreamhunters\n",
      "[18/20] Updated total_supply=20 for cardinallind\n",
      "[19/20] Updated total_supply=0 for bigcolection\n",
      "[20/20] Updated total_supply=25 for articulated1\n",
      "Done updating total_supply for Atomic collections.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    add_new_columns_if_not_exists(\n",
    "        db_path=\"sampling_frame.db\",\n",
    "        table_name=\"sampled_collections\"\n",
    "        )\n",
    "\n",
    "    update_atomic_collection_total_supply(\n",
    "        db_path=\"sampling_frame.db\",\n",
    "        table_name=\"sampled_collections\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1fdec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_atomic_nfts_table(db_path=\"sample_1.db\", table_name=\"atomic_nfts\"):\n",
    "    \"\"\"\n",
    "    Creates a table that stores item-level data from the AtomicAssets API.\n",
    "    Adjust columns as needed to match your chosen fields.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    c.execute(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        \n",
    "        -- references the original 'collection_name' from 'sampled_collections' (Atomic)\n",
    "        collection_name TEXT,\n",
    "        \n",
    "        asset_id TEXT,\n",
    "        owner TEXT,\n",
    "        item_name TEXT,\n",
    "        is_transferable INTEGER,\n",
    "        is_burnable INTEGER,\n",
    "        \n",
    "        template_id TEXT,        -- from template[\"template_id\"]\n",
    "        max_supply TEXT,         -- from template[\"max_supply\"]\n",
    "        issued_supply TEXT,      -- from template[\"issued_supply\"]\n",
    "        \n",
    "        minted_at_time TEXT,     -- item[\"minted_at_time\"]\n",
    "        minted_at_block TEXT,    -- item[\"minted_at_block\"]\n",
    "        \n",
    "        data_json TEXT,          -- optionally store the entire item if you like\n",
    "        fetched_at TEXT\n",
    "    )\n",
    "    \"\"\")\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51039b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_atomic_assets(collection_name, limit=10):\n",
    "    \"\"\"\n",
    "    Calls the endpoint:\n",
    "      GET /atomicassets/v1/assets?collection_name={collection_name}&limit={limit}\n",
    "    Returns a dict with \"success\", \"data\" (list), and \"query_time\".\n",
    "    We'll return the entire JSON for reference.\n",
    "    \"\"\"\n",
    "    base_url = \"https://wax.api.atomicassets.io/atomicassets/v1/assets\"\n",
    "    params = {\n",
    "        \"collection_name\": collection_name,\n",
    "        \"limit\": limit\n",
    "    }\n",
    "    resp = requests.get(base_url, params=params)\n",
    "    if resp.status_code == 200:\n",
    "        return resp.json()  # e.g. { \"success\": true, \"data\": [...], \"query_time\": 0 }\n",
    "    else:\n",
    "        print(f\"[Atomic NFT Fetch] Error {resp.status_code} => {resp.text}\")\n",
    "        return {}\n",
    "\n",
    "def store_atomic_nfts(db_path, table_name, coll_name, items):\n",
    "    \"\"\"\n",
    "    Inserts the provided item-level data into 'atomic_nfts'.\n",
    "    items is a list of dicts from data[\"data\"] in the AtomicAssets response.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    insert_sql = f\"\"\"\n",
    "    INSERT INTO {table_name} (\n",
    "        collection_name,\n",
    "        asset_id,\n",
    "        owner,\n",
    "        item_name,\n",
    "        is_transferable,\n",
    "        is_burnable,\n",
    "        template_id,\n",
    "        max_supply,\n",
    "        issued_supply,\n",
    "        minted_at_time,\n",
    "        minted_at_block,\n",
    "        data_json,\n",
    "        fetched_at\n",
    "    )\n",
    "    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, datetime('now'))\n",
    "    \"\"\"\n",
    "    \n",
    "    for item in items:\n",
    "        asset_id = item.get(\"asset_id\", \"\")\n",
    "        owner = item.get(\"owner\", \"\")\n",
    "        item_name = item.get(\"name\", \"\")\n",
    "        \n",
    "        # Convert booleans to int\n",
    "        is_transferable = 1 if item.get(\"is_transferable\", False) else 0\n",
    "        is_burnable = 1 if item.get(\"is_burnable\", False) else 0\n",
    "        \n",
    "        # Safely handle template, which can be None\n",
    "        template = item.get(\"template\") or {}\n",
    "        template_id = template.get(\"template_id\", \"\")\n",
    "        max_supply = template.get(\"max_supply\", \"\")\n",
    "        issued_supply = template.get(\"issued_supply\", \"\")\n",
    "        \n",
    "        minted_at_time = item.get(\"minted_at_time\", \"\")\n",
    "        minted_at_block = item.get(\"minted_at_block\", \"\")\n",
    "        \n",
    "        data_json = json.dumps(item)\n",
    "        \n",
    "        c.execute(insert_sql, (\n",
    "            coll_name,\n",
    "            asset_id,\n",
    "            owner,\n",
    "            item_name,\n",
    "            is_transferable,\n",
    "            is_burnable,\n",
    "            template_id,\n",
    "            max_supply,\n",
    "            issued_supply,\n",
    "            minted_at_time,\n",
    "            minted_at_block,\n",
    "            data_json\n",
    "        ))\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "def fetch_atomic_nfts_for_collections(db_path=\"sample_1.db\",\n",
    "                                      table_collections=\"sampled_collections\",\n",
    "                                      table_nfts=\"atomic_nfts\",\n",
    "                                      limit=10):\n",
    "    \"\"\"\n",
    "    1) Create the 'atomic_nfts' table if not exists.\n",
    "    2) Query 'sampled_collections' for marketplace='Atomic'.\n",
    "       We'll assume there's a column 'collection_name' for each row.\n",
    "    3) For each row, call fetch_atomic_assets(collection_name, limit={limit}),\n",
    "       then store them in 'atomic_nfts'.\n",
    "    \"\"\"\n",
    "    # 1) create table\n",
    "    create_atomic_nfts_table(db_path, table_nfts)\n",
    "    \n",
    "    # 2) get the relevant rows from 'sampled_collections'\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    c.execute(f\"\"\"\n",
    "    SELECT slug_name\n",
    "    FROM {table_collections}\n",
    "    WHERE marketplace='Atomic'\n",
    "      AND slug_name IS NOT NULL\n",
    "    \"\"\")\n",
    "    rows = c.fetchall()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"Found {len(rows)} Atomic collections to fetch up to {limit} assets each.\")\n",
    "    \n",
    "    count = 0\n",
    "    for (coll_name,) in rows:\n",
    "        print(f\"[{count+1}/{len(rows)}] Fetching {limit} assets for collection_name='{coll_name}'\")\n",
    "        \n",
    "        data = fetch_atomic_assets(coll_name, limit=limit)\n",
    "        items = data.get(\"data\", [])  # a list of item dicts\n",
    "        \n",
    "        store_atomic_nfts(db_path, table_nfts, coll_name, items)\n",
    "        count += 1\n",
    "    \n",
    "    print(\"Done fetching/storing Atomic NFTs in\", table_nfts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d6d7221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 Atomic collections to fetch up to 10 assets each.\n",
      "[1/20] Fetching 10 assets for collection_name='dimebagswaxd'\n",
      "[2/20] Fetching 10 assets for collection_name='alienwor1dsz'\n",
      "[3/20] Fetching 10 assets for collection_name='watercolorzz'\n",
      "[4/20] Fetching 10 assets for collection_name='adidasshoes3'\n",
      "[5/20] Fetching 10 assets for collection_name='varialandsio'\n",
      "[6/20] Fetching 10 assets for collection_name='waxwalletsto'\n",
      "[7/20] Fetching 10 assets for collection_name='ycjhajx5bcol'\n",
      "[8/20] Fetching 10 assets for collection_name='vikez1modern'\n",
      "[9/20] Fetching 10 assets for collection_name='nftartdesing'\n",
      "[10/20] Fetching 10 assets for collection_name='bestpepeever'\n",
      "[11/20] Fetching 10 assets for collection_name='agoratest123'\n",
      "[12/20] Fetching 10 assets for collection_name='oddsandendss'\n",
      "[13/20] Fetching 10 assets for collection_name='agruneforyou'\n",
      "[14/20] Fetching 10 assets for collection_name='1a2b3c4d5e11'\n",
      "[15/20] Fetching 10 assets for collection_name='targetedby11'\n",
      "[16/20] Fetching 10 assets for collection_name='ondjlfgjbuzp'\n",
      "[17/20] Fetching 10 assets for collection_name='dreamhunters'\n",
      "[18/20] Fetching 10 assets for collection_name='cardinallind'\n",
      "[19/20] Fetching 10 assets for collection_name='bigcolection'\n",
      "[20/20] Fetching 10 assets for collection_name='articulated1'\n",
      "Done fetching/storing Atomic NFTs in atomic_nfts\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    fetch_atomic_nfts_for_collections(\n",
    "        db_path=\"sampling_frame.db\",\n",
    "        table_collections=\"sampled_collections\",\n",
    "        table_nfts=\"atomic_nfts\",\n",
    "        limit=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7274e6b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
