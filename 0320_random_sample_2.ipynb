{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import sqlite3\n",
    "import random\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomized Sample 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_second_random_sample():\n",
    "    source_db = \"data/sampling_frame.db\"\n",
    "    first_sample_db = \"data/random_sample_1.db\"\n",
    "    second_sample_db = \"data/random_sample_2.db\"\n",
    "    table_name = \"sampled_collections\"\n",
    "    sample_size = 40\n",
    "\n",
    "    # 1. Load full dataset from source\n",
    "    conn_source = sqlite3.connect(source_db)\n",
    "    df_full = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn_source)\n",
    "    conn_source.close()\n",
    "\n",
    "    # 2. Load first random sample\n",
    "    conn_first = sqlite3.connect(first_sample_db)\n",
    "    df_first_sample = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn_first)\n",
    "    conn_first.close()\n",
    "\n",
    "    # 3. Exclude previously sampled collections using 'slug_name'\n",
    "    if \"slug_name\" in df_full.columns:\n",
    "        df_remaining = df_full[~df_full[\"slug_name\"].isin(df_first_sample[\"slug_name\"])]\n",
    "    else:\n",
    "        raise ValueError(\"Missing 'slug_name' column in source data.\")\n",
    "\n",
    "    print(f\"Remaining collections after exclusion: {len(df_remaining)}\")\n",
    "\n",
    "    # 4. Draw the new sample\n",
    "    df_second_sample = df_remaining.sample(n=sample_size, random_state=123)\n",
    "    print(f\"Sampled {len(df_second_sample)} collections for second sample.\")\n",
    "\n",
    "    # 5. Write to new database\n",
    "    conn_second = sqlite3.connect(second_sample_db)\n",
    "    df_second_sample.to_sql(table_name, conn_second, if_exists=\"replace\", index=False)\n",
    "    conn_second.close()\n",
    "\n",
    "    print(f\"Second sample saved to '{second_sample_db}' in table '{table_name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining collections after exclusion: 79\n",
      "Sampled 40 collections for second sample.\n",
      "Second sample saved to 'random_sample_2.db' in table 'sampled_collections'.\n"
     ]
    }
   ],
   "source": [
    "draw_second_random_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped table: opensea_nfts\n",
      "Dropped table: rarible_nfts\n",
      "Dropped table: magiceden_nfts\n",
      "Dropped table: atomic_nfts\n",
      "All tables except 'sampled_collections' and 'sqlite_sequence' have been dropped from 'random_sample_2.db'.\n"
     ]
    }
   ],
   "source": [
    "def drop_other_tables(db_path=\"data/random_sample_2.db\", keep_table=\"sampled_collections\"):\n",
    "    \"\"\"\n",
    "    Connects to 'db_path', enumerates all tables, \n",
    "    and drops any table that is not 'keep_table' or 'sqlite_sequence'.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # 1) Get all tables in the SQLite file\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "    tables = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "    # 2) For each table, if it's not keep_table or 'sqlite_sequence', drop it\n",
    "    for tbl in tables:\n",
    "        if tbl not in [keep_table, \"sqlite_sequence\"]:\n",
    "            drop_sql = f\"DROP TABLE IF EXISTS '{tbl}'\"\n",
    "            cursor.execute(drop_sql)\n",
    "            print(f\"Dropped table: {tbl}\")\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(f\"All tables except '{keep_table}' and 'sqlite_sequence' have been dropped from '{db_path}'.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    drop_other_tables(db_path=\"data/random_sample_2.db\", keep_table=\"sampled_collections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_opensea_nfts_table(db_path=\"data/random_sample_2.db\", table_name=\"opensea_nfts\"):\n",
    "    \"\"\"\n",
    "    Creates a table with columns for the relevant fields from the OpenSea NFT JSON.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    # Create table if not exists\n",
    "    c.execute(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        \n",
    "        -- Linking field from 'sampled_collections' table\n",
    "        slug_name TEXT,\n",
    "        \n",
    "        -- Fields from the NFT JSON:\n",
    "        identifier TEXT,\n",
    "        contract TEXT,\n",
    "        token_standard TEXT,\n",
    "        name TEXT,\n",
    "        description TEXT,\n",
    "        image_url TEXT,\n",
    "        display_image_url TEXT,\n",
    "        display_animation_url TEXT,\n",
    "        metadata_url TEXT,\n",
    "        opensea_url TEXT,\n",
    "        updated_at TEXT,\n",
    "        is_disabled INTEGER,\n",
    "        is_nsfw INTEGER,\n",
    "        \n",
    "        fetched_at TEXT\n",
    "    )\n",
    "    \"\"\")\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_opensea_nfts(collection_slug, api_key, limit=10):\n",
    "    \"\"\"\n",
    "    Calls the OpenSea endpoint:\n",
    "      GET /api/v2/collection/{collection_slug}/nfts?limit={limit}\n",
    "    Returns the parsed JSON dict with:\n",
    "       { \"nfts\": [ { ... }, {...} ], \"next\": \"string\" }\n",
    "    If there's an error, returns an empty dict.\n",
    "    \"\"\"\n",
    "    base_url = \"https://api.opensea.io/api/v2/collection\"\n",
    "    url = f\"{base_url}/{collection_slug}/nfts\"\n",
    "    \n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"x-api-key\": api_key\n",
    "    }\n",
    "    params = {\n",
    "        \"limit\": limit\n",
    "    }\n",
    "    \n",
    "    resp = requests.get(url, headers=headers, params=params)\n",
    "    if resp.status_code == 200:\n",
    "        return resp.json()\n",
    "    else:\n",
    "        print(f\"[OpenSea NFT Fetch] Error {resp.status_code} for '{collection_slug}': {resp.text}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def store_opensea_nfts(db_path, table_name, slug_name, nfts_data):\n",
    "    \"\"\"\n",
    "    Inserts each NFT from nfts_data[\"nfts\"] into opensea_nfts, linking them\n",
    "    via 'slug_name'. We do NOT store the 'collection' field from the JSON,\n",
    "    but instead rely on 'slug_name' to identify the parent collection.\n",
    "    \"\"\"\n",
    "    nfts_list = nfts_data.get(\"nfts\", [])\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    insert_sql = f\"\"\"\n",
    "    INSERT INTO {table_name} (\n",
    "        slug_name,\n",
    "        identifier,\n",
    "        contract,\n",
    "        token_standard,\n",
    "        name,\n",
    "        description,\n",
    "        image_url,\n",
    "        display_image_url,\n",
    "        display_animation_url,\n",
    "        metadata_url,\n",
    "        opensea_url,\n",
    "        updated_at,\n",
    "        is_disabled,\n",
    "        is_nsfw,\n",
    "        fetched_at\n",
    "    )\n",
    "    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, datetime('now'))\n",
    "    \"\"\"\n",
    "    \n",
    "    for nft in nfts_list:\n",
    "        identifier = nft.get(\"identifier\", \"\")\n",
    "        contract = nft.get(\"contract\", \"\")\n",
    "        token_standard = nft.get(\"token_standard\", \"\")\n",
    "        name = nft.get(\"name\", \"\")\n",
    "        desc = nft.get(\"description\", \"\")\n",
    "        image_url = nft.get(\"image_url\", \"\")\n",
    "        display_image = nft.get(\"display_image_url\", \"\")\n",
    "        display_animation = nft.get(\"display_animation_url\", \"\")\n",
    "        metadata_url = nft.get(\"metadata_url\", \"\")\n",
    "        opensea_url = nft.get(\"opensea_url\", \"\")\n",
    "        updated_at = nft.get(\"updated_at\", \"\")\n",
    "        \n",
    "        # Convert booleans to int (1/0)\n",
    "        is_disabled = 1 if nft.get(\"is_disabled\", False) else 0\n",
    "        is_nsfw = 1 if nft.get(\"is_nsfw\", False) else 0\n",
    "        \n",
    "        c.execute(insert_sql, (\n",
    "            slug_name,\n",
    "            identifier,\n",
    "            contract,\n",
    "            token_standard,\n",
    "            name,\n",
    "            desc,\n",
    "            image_url,\n",
    "            display_image,\n",
    "            display_animation,\n",
    "            metadata_url,\n",
    "            opensea_url,\n",
    "            updated_at,\n",
    "            is_disabled,\n",
    "            is_nsfw\n",
    "        ))\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def fetch_opensea_nfts_for_collections(db_path=\"data/random_sample_2.db\", table_collections=\"sampled_collections\",\n",
    "                                       table_nfts=\"opensea_nfts\", api_key=\"YOUR_API_KEY\", limit=10):\n",
    "    \"\"\"\n",
    "    1) Ensure 'opensea_nfts' table exists.\n",
    "    2) Query 'sampled_collections' for marketplace='OpenSea', using 'slug_name'\n",
    "    3) For each, fetch up to {limit} items from OpenSea.\n",
    "    4) Store them in 'opensea_nfts'.\n",
    "    \"\"\"\n",
    "    # Create table if not exists\n",
    "    create_opensea_nfts_table(db_path, table_nfts)\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    # Query the relevant rows\n",
    "    c.execute(f\"\"\"\n",
    "    SELECT slug_name\n",
    "    FROM {table_collections}\n",
    "    WHERE marketplace='OpenSea'\n",
    "      AND slug_name IS NOT NULL\n",
    "    \"\"\")\n",
    "    rows = c.fetchall()\n",
    "    conn.close()\n",
    "    \n",
    "    for idx, (slug,) in enumerate(rows, start=1):\n",
    "        print(f\"[{idx}/{len(rows)}] Fetching up to {limit} NFTs for slug='{slug}'\")\n",
    "        nfts_data = fetch_opensea_nfts(slug, api_key, limit=limit)\n",
    "        store_opensea_nfts(db_path, table_nfts, slug, nfts_data)\n",
    "    \n",
    "    print(\"Done fetching NFT data for all OpenSea collections in sample_5.db.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/11] Fetching up to 10 NFTs for slug='alpha-mystic-collection'\n",
      "[2/11] Fetching up to 10 NFTs for slug='competitors-iso'\n",
      "[3/11] Fetching up to 10 NFTs for slug='reliable-susda-1'\n",
      "[4/11] Fetching up to 10 NFTs for slug='wrong-badge-rewards'\n",
      "[5/11] Fetching up to 10 NFTs for slug='shepherd-20'\n",
      "[6/11] Fetching up to 10 NFTs for slug='my-world-73'\n",
      "[7/11] Fetching up to 10 NFTs for slug='alphaswap'\n",
      "[8/11] Fetching up to 10 NFTs for slug='samuderakepri-adventure-pass'\n",
      "[9/11] Fetching up to 10 NFTs for slug='loresbt-2'\n",
      "[10/11] Fetching up to 10 NFTs for slug='summer-165'\n",
      "[11/11] Fetching up to 10 NFTs for slug='flora-fauna-frames'\n",
      "Done fetching NFT data for all OpenSea collections in sample_5.db.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    OPENSEA_API_KEY = \"your_key\"\n",
    "    fetch_opensea_nfts_for_collections(\n",
    "        db_path=\"data/random_sample_2.db\",\n",
    "        table_collections=\"sampled_collections\",\n",
    "        table_nfts=\"opensea_nfts\",\n",
    "        api_key=OPENSEA_API_KEY,\n",
    "        limit=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_opensea_collection_details(slug, api_key):\n",
    "    \"\"\"\n",
    "    Calls the endpoint:\n",
    "      GET https://api.opensea.io/api/v2/collections/{collection_slug}\n",
    "    Returns a dict with keys:\n",
    "       'collection', 'name', 'total_supply', 'created_date', 'fees' (list), etc.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.opensea.io/api/v2/collections/{slug}\"\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"x-api-key\": api_key\n",
    "    }\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    if resp.status_code == 200:\n",
    "        return resp.json() \n",
    "    else:\n",
    "        print(f\"[OpenSea Collection Fetch] Error {resp.status_code} for slug='{slug}': {resp.text}\")\n",
    "        return {}\n",
    "\n",
    "def update_opensea_collection_data(db_path, table_name, slug, data):\n",
    "    \"\"\"\n",
    "    data has: 'created_date', 'total_supply', 'fees' list, etc.\n",
    "    We map:\n",
    "       created_date -> created_time\n",
    "       total_supply -> total_supply\n",
    "       fees[0].fee -> marketplace_fee\n",
    "       fees[1].fee -> royalty_fee (if exist)\n",
    "    Then do UPDATE on the row with marketplace='OpenSea' AND slug_name=?\n",
    "    \"\"\"\n",
    "    collection_slug = data.get(\"collection\", slug)  # fallback to slug if missing\n",
    "    created_date = data.get(\"created_date\", None)   # string\n",
    "    total_supply = data.get(\"total_supply\", 0.0)\n",
    "    fees = data.get(\"fees\", [])\n",
    "    \n",
    "    marketplace_fee = None\n",
    "    royalty_fee = None\n",
    "    \n",
    "    if len(fees) >= 1:\n",
    "        # first fee => marketplace fee\n",
    "        marketplace_fee = fees[0].get(\"fee\", 0.0)\n",
    "    if len(fees) >= 2:\n",
    "        # second fee => royalty fee\n",
    "        royalty_fee = fees[1].get(\"fee\", 0.0)\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    update_stmt = f\"\"\"\n",
    "    UPDATE {table_name}\n",
    "    SET created_time = ?,\n",
    "        total_supply = ?,\n",
    "        marketplace_fee = ?,\n",
    "        royalty_fee = ?\n",
    "    WHERE marketplace='OpenSea'\n",
    "      AND slug_name=?\n",
    "    \"\"\"\n",
    "    c.execute(update_stmt, (\n",
    "        created_date,\n",
    "        total_supply,\n",
    "        marketplace_fee,\n",
    "        royalty_fee,\n",
    "        slug\n",
    "    ))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def fetch_and_update_opensea_collection_data(db_path=\"sample_5.db\", table_name=\"sampled_collections\",\n",
    "                                             api_key=\"YOUR_API_KEY\"):\n",
    "    \"\"\"\n",
    "    # 1) Ensure the new columns exist in the 'sampled_collections' table.\n",
    "    # 2) Query all rows for marketplace='OpenSea'.\n",
    "    # 3) For each slug_name, call the new endpoint to fetch additional data,\n",
    "    #    then update the row with created_time, total_supply, marketplace_fee, royalty_fee.\n",
    "    # \"\"\"\n",
    "    \n",
    "    # 2) Query the existing table\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    c.execute(f\"\"\"\n",
    "        SELECT slug_name\n",
    "        FROM {table_name}\n",
    "        WHERE marketplace='OpenSea'\n",
    "          AND slug_name IS NOT NULL\n",
    "    \"\"\")\n",
    "    rows = c.fetchall()\n",
    "    conn.close()\n",
    "    \n",
    "    for idx, (slug,) in enumerate(rows, start=1):\n",
    "        print(f\"[{idx}/{len(rows)}] Fetching extended data for slug='{slug}'\")\n",
    "        data = fetch_opensea_collection_details(slug, api_key)\n",
    "        if data:\n",
    "            # parse JSON: top-level fields => data\n",
    "            # The endpoint returns { \"collection\": \"string\", \"name\": \"string\", ...}\n",
    "            # We just pass it to update\n",
    "            update_opensea_collection_data(db_path, table_name, slug, data)\n",
    "            \n",
    "        # optional short sleep to avoid rate-limiting\n",
    "        # time.sleep(0.2)\n",
    "\n",
    "    print(\"Done updating additional data for all OpenSea collections in random_sample_2.db.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/11] Fetching extended data for slug='alpha-mystic-collection'\n",
      "[2/11] Fetching extended data for slug='competitors-iso'\n",
      "[3/11] Fetching extended data for slug='reliable-susda-1'\n",
      "[4/11] Fetching extended data for slug='wrong-badge-rewards'\n",
      "[5/11] Fetching extended data for slug='shepherd-20'\n",
      "[6/11] Fetching extended data for slug='my-world-73'\n",
      "[7/11] Fetching extended data for slug='alphaswap'\n",
      "[8/11] Fetching extended data for slug='samuderakepri-adventure-pass'\n",
      "[9/11] Fetching extended data for slug='loresbt-2'\n",
      "[10/11] Fetching extended data for slug='summer-165'\n",
      "[11/11] Fetching extended data for slug='flora-fauna-frames'\n",
      "Done updating additional data for all OpenSea collections in random_sample_2.db.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    OPENSEA_API_KEY = \"your_key\"\n",
    "    fetch_and_update_opensea_collection_data(\n",
    "        db_path=\"data/random_sample_2.db\",\n",
    "        table_name=\"sampled_collections\",\n",
    "        api_key=OPENSEA_API_KEY\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_traits_json_column_if_not_exists(db_path=\"data/random_sample_2.db\", table_name=\"sampled_collections\"):\n",
    "    \"\"\"\n",
    "    Adds a 'traits_json' column to the 'sampled_collections' table if it does not exist.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    # Attempt an ALTER TABLE for 'traits_json'\n",
    "    alter_stmt = f\"ALTER TABLE {table_name} ADD COLUMN traits_json TEXT\"\n",
    "    try:\n",
    "        c.execute(alter_stmt)\n",
    "        print(f\"Added 'traits_json' column to {table_name}\")\n",
    "    except sqlite3.OperationalError:\n",
    "        # Column probably already exists\n",
    "        pass\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_opensea_traits(collection_slug, api_key):\n",
    "    \"\"\"\n",
    "    Calls the OpenSea endpoint:\n",
    "    GET /api/v2/traits/{collection_slug}\n",
    "    \n",
    "    Returns JSON like:\n",
    "      {\n",
    "        \"categories\": {...},\n",
    "        \"counts\": {...}\n",
    "      }\n",
    "    \"\"\"\n",
    "    url = f\"https://api.opensea.io/api/v2/traits/{collection_slug}\"\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"x-api-key\": api_key\n",
    "    }\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    if resp.status_code == 200:\n",
    "        return resp.json()\n",
    "    else:\n",
    "        print(f\"[OpenSea Traits] Error {resp.status_code} for slug='{collection_slug}': {resp.text}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def store_traits_json_in_collections(db_path, table_name, slug_name, traits_data):\n",
    "    \"\"\"\n",
    "    Updates the 'sampled_collections' table with the entire traits JSON\n",
    "    in the 'traits_json' column for the given slug_name.\n",
    "    \n",
    "    We assume marketplace='OpenSea' is the row filter.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    update_stmt = f\"\"\"\n",
    "    UPDATE {table_name}\n",
    "    SET traits_json = ?\n",
    "    WHERE marketplace='OpenSea'\n",
    "      AND slug_name=?\n",
    "    \"\"\"\n",
    "    c.execute(update_stmt, (json.dumps(traits_data), slug_name))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def fetch_and_update_opensea_traits_in_collections(db_path=\"sample_5.db\",\n",
    "                                                   table_name=\"sampled_collections\",\n",
    "                                                   api_key=\"YOUR_API_KEY\"):\n",
    "    \"\"\"\n",
    "    1) Ensure 'traits_json' column exists in 'sampled_collections'.\n",
    "    2) Query all rows where marketplace='OpenSea' and slug_name is not null.\n",
    "    3) For each slug_name, fetch traits JSON from /api/v2/traits/{slug_name} and\n",
    "       store it in the 'traits_json' column.\n",
    "    \"\"\"\n",
    "    # 1) Add 'traits_json' column if needed\n",
    "    add_traits_json_column_if_not_exists(db_path, table_name)\n",
    "    \n",
    "    # 2) Query the existing table for OpenSea slugs\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    c.execute(f\"\"\"\n",
    "    SELECT slug_name\n",
    "    FROM {table_name}\n",
    "    WHERE marketplace='OpenSea'\n",
    "      AND slug_name IS NOT NULL\n",
    "    \"\"\")\n",
    "    rows = c.fetchall()\n",
    "    conn.close()\n",
    "    \n",
    "    # 3) For each slug, fetch traits and store\n",
    "    for idx, (slug,) in enumerate(rows, start=1):\n",
    "        print(f\"[{idx}/{len(rows)}] Fetching traits for slug='{slug}'\")\n",
    "        traits_data = fetch_opensea_traits(slug, api_key)\n",
    "        store_traits_json_in_collections(db_path, table_name, slug, traits_data)\n",
    "    \n",
    "    print(f\"Done updating 'traits_json' for all OpenSea collections in {table_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/11] Fetching traits for slug='alpha-mystic-collection'\n",
      "[2/11] Fetching traits for slug='competitors-iso'\n",
      "[3/11] Fetching traits for slug='reliable-susda-1'\n",
      "[4/11] Fetching traits for slug='wrong-badge-rewards'\n",
      "[5/11] Fetching traits for slug='shepherd-20'\n",
      "[6/11] Fetching traits for slug='my-world-73'\n",
      "[7/11] Fetching traits for slug='alphaswap'\n",
      "[8/11] Fetching traits for slug='samuderakepri-adventure-pass'\n",
      "[9/11] Fetching traits for slug='loresbt-2'\n",
      "[10/11] Fetching traits for slug='summer-165'\n",
      "[11/11] Fetching traits for slug='flora-fauna-frames'\n",
      "Done updating 'traits_json' for all OpenSea collections in sampled_collections.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    OPENSEA_API_KEY = \"your_key\"\n",
    "    fetch_and_update_opensea_traits_in_collections(\n",
    "        db_path=\"data/random_sample_2.db\",\n",
    "        table_name=\"sampled_collections\",\n",
    "        api_key=OPENSEA_API_KEY\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rarible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_collection_id(collection_id_blockchain):\n",
    "    \"\"\"\n",
    "    Given a string like '0x594824a3d6e5777b3c7cc202ad1050435aac7698:ethereum',\n",
    "    transform it into 'ETHEREUM:0x594824a3d6e5777b3c7cc202ad1050435aac7698'\n",
    "    for Rarible's endpoint: ?collectionIds=ETHEREUM%3A0x..\n",
    "    \"\"\"\n",
    "    if \":\" not in collection_id_blockchain:\n",
    "        return None\n",
    "    coll_id, chain = collection_id_blockchain.split(\":\")\n",
    "    return f\"{chain.upper()}:{coll_id}\"\n",
    "\n",
    "def fetch_rarible_traits_single(collection_id_rarible, api_key):\n",
    "    \"\"\"\n",
    "    Calls Rarible's endpoint, single collection ID:\n",
    "      GET /v0.1/items/traits?collectionIds={collection_id_rarible}\n",
    "\n",
    "    Returns the JSON with structure:\n",
    "      {\n",
    "        \"continuation\": \"string\",\n",
    "        \"traits\": [ { \"key\": {...}, \"values\": [...] }, ... ]\n",
    "      }\n",
    "    \"\"\"\n",
    "    base_url = \"https://api.rarible.org/v0.1/items/traits\"\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"X-API-KEY\": api_key\n",
    "    }\n",
    "    # For a single ID, just pass collectionIds once\n",
    "    params = {\n",
    "        \"collectionIds\": collection_id_rarible\n",
    "    }\n",
    "    resp = requests.get(base_url, headers=headers, params=params)\n",
    "    if resp.status_code == 200:\n",
    "        return resp.json()\n",
    "    else:\n",
    "        print(f\"[Rarible Traits] Error {resp.status_code} => {resp.text}\")\n",
    "        return {}\n",
    "\n",
    "def update_traits_json_for_rarible(db_path=\"data/random_sample_2.db\",\n",
    "                                   table_name=\"sampled_collections\",\n",
    "                                   api_key=\"YOUR_RARIBLE_API_KEY\"):\n",
    "    \"\"\"\n",
    "    1) Query 'sampled_collections' where marketplace='Rarible'\n",
    "    2) For each row, transform 'collection_id' to Rarible format, fetch traits,\n",
    "       store entire JSON in 'traits_json' column\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    # We'll assume 'traits_json' column already exists (like for OpenSea).\n",
    "    # If not, you'll need the add_column logic from your previous script.\n",
    "    \n",
    "    c.execute(f\"\"\"\n",
    "    SELECT collection_id\n",
    "    FROM {table_name}\n",
    "    WHERE marketplace='Rarible'\n",
    "      AND collection_id IS NOT NULL\n",
    "    \"\"\")\n",
    "    rows = c.fetchall()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"Found {len(rows)} Rarible collections to fetch traits for.\")\n",
    "    \n",
    "    count = 0\n",
    "    for (orig_id,) in rows:\n",
    "        rarible_id = transform_collection_id(orig_id)\n",
    "        if rarible_id is None:\n",
    "            print(f\"Skipping invalid format => {orig_id}\")\n",
    "            continue\n",
    "        \n",
    "        data = fetch_rarible_traits_single(rarible_id, api_key)\n",
    "        \n",
    "        # Now update the 'traits_json' in that row\n",
    "        conn2 = sqlite3.connect(db_path)\n",
    "        c2 = conn2.cursor()\n",
    "        update_stmt = f\"\"\"\n",
    "        UPDATE {table_name}\n",
    "        SET traits_json = ?\n",
    "        WHERE marketplace='Rarible'\n",
    "          AND collection_id=?\n",
    "        \"\"\"\n",
    "        c2.execute(update_stmt, (json.dumps(data), orig_id))\n",
    "        conn2.commit()\n",
    "        conn2.close()\n",
    "        \n",
    "        count += 1\n",
    "        print(f\"[{count}/{len(rows)}] Updated 'traits_json' for => {orig_id}\")\n",
    "    \n",
    "    print(\"Done fetching/storing Rarible traits in 'traits_json' column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 Rarible collections to fetch traits for.\n",
      "[1/22] Updated 'traits_json' for => 0x5194161b237be56026edc9230a0a0ad859746bb5:polygon\n",
      "[2/22] Updated 'traits_json' for => 0xa8b3e3eade7686158bad5a5d066ce6fed27cb802:base\n",
      "[3/22] Updated 'traits_json' for => 0x47339e603a5091c262f29a4a393d39762fe0c7de:base\n",
      "[4/22] Updated 'traits_json' for => 0xa10d31341ddcc7d848626fe405bc208d5f8b64a4:arbitrum\n",
      "[5/22] Updated 'traits_json' for => 0xc53b1b9e4765132b1bad688cec0e2f4b64c0131c:polygon\n",
      "[6/22] Updated 'traits_json' for => 0x8aa932f0e1daa959bdcedc7b696a96908dbf091e:base\n",
      "[7/22] Updated 'traits_json' for => 0xcca474855e1c57704abb5bc5e7fa7fa34b71e922:arbitrum\n",
      "[8/22] Updated 'traits_json' for => 0x967fd257f8978b3338df1879baf074837cf2d853:ethereum\n",
      "[9/22] Updated 'traits_json' for => 0x09bd3e86d019c3520f51305e2cc08d93d4f66546:ethereum\n",
      "[10/22] Updated 'traits_json' for => 0x10ff1777808e4ae4f93af45d88e4a12599153da4:polygon\n",
      "[11/22] Updated 'traits_json' for => 0xf1f7c338da9e92b95489187bc145d31e781063d6:base\n",
      "[12/22] Updated 'traits_json' for => 0xbdf2cefc92dde9a9e276d7c710fdaa0f28663af1:base\n",
      "[13/22] Updated 'traits_json' for => 0x57e104e68bbca6565486848349994d754ad61866:base\n",
      "[14/22] Updated 'traits_json' for => 0xa3330ca4cd7ce3db9f6ca691b94f49c503dedca300ec6599cafa7e577ac946fc:aptos\n",
      "[15/22] Updated 'traits_json' for => 0x5af0171ae867290357f2aef703a163f10f4b74b144534941457d3f5e217cf02c:aptos\n",
      "[16/22] Updated 'traits_json' for => 0xa891b9211ce3b2a5bfc6f8bdbd837cda0641379a:ethereum\n",
      "[17/22] Updated 'traits_json' for => 0xc219d80cac121b59c232041f72819d21e12e223ce8c543a7609a177b6fd16a0f:aptos\n",
      "[18/22] Updated 'traits_json' for => 0x1195cf65f83b3a5768f3c496d3a05ad6412c64b7:etherlink\n",
      "[19/22] Updated 'traits_json' for => 0x00b8f905c1ece49f78676733073e308fde4da737:base\n",
      "[20/22] Updated 'traits_json' for => 0x93eca311f3a6f80df3a21a85fcfceeecf97f644b:polygon\n",
      "[21/22] Updated 'traits_json' for => 0x23b5ba0f50664ce3a48ded214e773eb430818e65:celo\n",
      "[22/22] Updated 'traits_json' for => 0x469f7afa52541afb334373368bf6d8dce891f219:base\n",
      "Done fetching/storing Rarible traits in 'traits_json' column.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    RARIBLE_API_KEY = \"your_key\"\n",
    "    update_traits_json_for_rarible(\n",
    "        db_path=\"data/random_sample_2.db\",\n",
    "        table_name=\"sampled_collections\",\n",
    "        api_key=RARIBLE_API_KEY\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rarible_nfts_table(db_path=\"data/random_sample_2.db\", table_name=\"rarible_nfts\"):\n",
    "    \"\"\"\n",
    "    Creates a table 'rarible_nfts' with columns similar to 'opensea_nfts',\n",
    "    storing key fields from the Rarible item-level data plus a link back \n",
    "    to 'collection_id' in 'sampled_collections'.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    c.execute(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        -- references the original 'id:blockchain' format from 'sampled_collections'\n",
    "        collection_id TEXT,\n",
    "        \n",
    "        item_id TEXT,          -- item[\"id\"] e.g. \"ETHEREUM:0xb66a6...:123\"\n",
    "        blockchain TEXT,       -- item[\"blockchain\"]\n",
    "        contract TEXT,         -- item[\"contract\"]\n",
    "        token_id TEXT,         -- item[\"tokenId\"]\n",
    "        \n",
    "        name TEXT,             -- from item[\"meta\"][\"name\"]\n",
    "        description TEXT,      -- from item[\"meta\"][\"description\"]\n",
    "        image_url TEXT,        -- optional, if we parse item[\"meta\"][\"content\"] for an image\n",
    "        minted_at TEXT,        -- item[\"mintedAt\"]\n",
    "        last_updated TEXT,     -- item[\"lastUpdatedAt\"]\n",
    "        supply REAL,           -- item[\"supply\"] \n",
    "        owner_if_single TEXT,  -- item[\"ownerIfSingle\"] if we want\n",
    "        \n",
    "        project_url TEXT,      -- e.g. item[\"meta\"][\"externalUri\"] if present\n",
    "        created_at TEXT,       -- item[\"meta\"][\"createdAt\"]\n",
    "        \n",
    "        fetched_at TEXT\n",
    "    )\n",
    "    \"\"\")\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_collection_id_for_rarible(orig_id):\n",
    "    \"\"\"\n",
    "    Convert 'id:blockchain' => 'BLOCKCHAIN:0xid' for use in Rarible API.\n",
    "    e.g. '0x5948abcd:ethereum' => 'ETHEREUM:0x5948abcd'\n",
    "    \"\"\"\n",
    "    if \":\" not in orig_id:\n",
    "        return None\n",
    "    coll_id, chain = orig_id.split(\":\")\n",
    "    return f\"{chain.upper()}:{coll_id}\"\n",
    "\n",
    "def fetch_rarible_nfts(collection_param, api_key, size=10):\n",
    "    \"\"\"\n",
    "    Calls Rarible endpoint:\n",
    "      GET /v0.1/items/byCollection?collection={collection_param}&size={size}\n",
    "    e.g. collection_param = 'ETHEREUM:0xb66a603f...'\n",
    "    Returns a dict with keys 'continuation' and 'items' (list).\n",
    "    \"\"\"\n",
    "    url = \"https://api.rarible.org/v0.1/items/byCollection\"\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"X-API-KEY\": api_key\n",
    "    }\n",
    "    params = {\n",
    "        \"collection\": collection_param,\n",
    "        \"size\": size\n",
    "    }\n",
    "    resp = requests.get(url, headers=headers, params=params)\n",
    "    if resp.status_code == 200:\n",
    "        return resp.json()\n",
    "    else:\n",
    "        print(f\"[Rarible NFT Fetch] Error {resp.status_code} => {resp.text}\")\n",
    "        return {}\n",
    "\n",
    "def store_rarible_nfts(db_path, table_name, orig_id, items_list):\n",
    "    \"\"\"\n",
    "    Insert up to 10 items into 'rarible_nfts' table, storing relevant fields.\n",
    "    :param orig_id: The original 'id:blockchain' used in 'sampled_collections'\n",
    "    :param items_list: list of item dicts from the Rarible API\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    insert_sql = f\"\"\"\n",
    "    INSERT INTO {table_name} (\n",
    "        collection_id,\n",
    "        item_id,\n",
    "        blockchain,\n",
    "        contract,\n",
    "        token_id,\n",
    "        name,\n",
    "        description,\n",
    "        image_url,\n",
    "        minted_at,\n",
    "        last_updated,\n",
    "        supply,\n",
    "        owner_if_single,\n",
    "        project_url,\n",
    "        created_at,\n",
    "        fetched_at\n",
    "    )\n",
    "    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, datetime('now'))\n",
    "    \"\"\"\n",
    "    \n",
    "    for item in items_list:\n",
    "        item_id = item.get(\"id\", \"\")\n",
    "        blockchain = item.get(\"blockchain\", \"\")\n",
    "        contract = item.get(\"contract\", \"\")\n",
    "        token_id = str(item.get(\"tokenId\", \"\"))\n",
    "        supply = item.get(\"supply\", 0)\n",
    "        minted_at = item.get(\"mintedAt\", \"\")\n",
    "        last_updated = item.get(\"lastUpdatedAt\", \"\")\n",
    "        owner_if_single = item.get(\"ownerIfSingle\", \"\")\n",
    "        \n",
    "        meta = item.get(\"meta\", {})\n",
    "        name = meta.get(\"name\", \"\")\n",
    "        description = meta.get(\"description\", \"\")\n",
    "        created_at = meta.get(\"createdAt\", \"\")\n",
    "        project_url = meta.get(\"externalUri\", \"\")\n",
    "        \n",
    "        # Optional: parse meta[\"content\"] for image\n",
    "        # The example doesn't show a direct \"url\", so we may store 'N/A' or parse further\n",
    "        content_list = meta.get(\"content\", [])\n",
    "        image_url = \"N/A\"\n",
    "        for content_obj in content_list:\n",
    "            # if there's a recognized URL, parse it\n",
    "            # The example lacks a 'url' field, so we skip. \n",
    "            # If your real data has it, you might do:\n",
    "            # image_url = content_obj.get(\"url\", \"N/A\")\n",
    "            # break\n",
    "            pass\n",
    "        \n",
    "        c.execute(insert_sql, (\n",
    "            orig_id,      # collection_id (as it appears in 'sampled_collections')\n",
    "            item_id,\n",
    "            blockchain,\n",
    "            contract,\n",
    "            token_id,\n",
    "            name,\n",
    "            description,\n",
    "            image_url,\n",
    "            minted_at,\n",
    "            last_updated,\n",
    "            supply,\n",
    "            owner_if_single,\n",
    "            project_url,\n",
    "            created_at\n",
    "        ))\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def fetch_rarible_nfts_for_collections(db_path=\"data/random_sample_2.db\", table_collections=\"sampled_collections\",\n",
    "                                       table_nfts=\"rarible_nfts\", api_key=\"YOUR_RARIBLE_API_KEY\", size=10):\n",
    "    \"\"\"\n",
    "    1) Create the 'rarible_nfts' table if not exists.\n",
    "    2) Query 'sampled_collections' where marketplace='Rarible'.\n",
    "    3) Transform 'collection_id:blockchain' to 'BLOCKCHAIN:collection_id'.\n",
    "    4) Fetch items (size=10) from Rarible's endpoint.\n",
    "    5) Store them in 'rarible_nfts'.\n",
    "    \"\"\"\n",
    "    # 1) create table\n",
    "    create_rarible_nfts_table(db_path, table_nfts)\n",
    "    \n",
    "    # 2) query the relevant rows\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    c.execute(f\"\"\"\n",
    "    SELECT collection_id\n",
    "    FROM {table_collections}\n",
    "    WHERE marketplace='Rarible'\n",
    "      AND collection_id IS NOT NULL\n",
    "    \"\"\")\n",
    "    rows = c.fetchall()\n",
    "    conn.close()\n",
    "    \n",
    "    count = 0\n",
    "    for (orig_id,) in rows:\n",
    "        rarible_format = transform_collection_id_for_rarible(orig_id)\n",
    "        if not rarible_format:\n",
    "            print(f\"Skipping invalid format => {orig_id}\")\n",
    "            continue\n",
    "        \n",
    "        data = fetch_rarible_nfts(rarible_format, api_key, size=size)\n",
    "        items = data.get(\"items\", [])\n",
    "        \n",
    "        store_rarible_nfts(db_path, table_nfts, orig_id, items)\n",
    "        count += 1\n",
    "        print(f\"[{count}/{len(rows)}] Inserted up to {len(items)} items for {orig_id}\")\n",
    "    \n",
    "    print(\"Done fetching and storing Rarible NFTs in\", table_nfts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/22] Inserted up to 4 items for 0x5194161b237be56026edc9230a0a0ad859746bb5:polygon\n",
      "[2/22] Inserted up to 0 items for 0xa8b3e3eade7686158bad5a5d066ce6fed27cb802:base\n",
      "[3/22] Inserted up to 1 items for 0x47339e603a5091c262f29a4a393d39762fe0c7de:base\n",
      "[4/22] Inserted up to 1 items for 0xa10d31341ddcc7d848626fe405bc208d5f8b64a4:arbitrum\n",
      "[5/22] Inserted up to 9 items for 0xc53b1b9e4765132b1bad688cec0e2f4b64c0131c:polygon\n",
      "[6/22] Inserted up to 10 items for 0x8aa932f0e1daa959bdcedc7b696a96908dbf091e:base\n",
      "[7/22] Inserted up to 0 items for 0xcca474855e1c57704abb5bc5e7fa7fa34b71e922:arbitrum\n",
      "[8/22] Inserted up to 1 items for 0x967fd257f8978b3338df1879baf074837cf2d853:ethereum\n",
      "[9/22] Inserted up to 4 items for 0x09bd3e86d019c3520f51305e2cc08d93d4f66546:ethereum\n",
      "[10/22] Inserted up to 10 items for 0x10ff1777808e4ae4f93af45d88e4a12599153da4:polygon\n",
      "[11/22] Inserted up to 1 items for 0xf1f7c338da9e92b95489187bc145d31e781063d6:base\n",
      "[12/22] Inserted up to 10 items for 0xbdf2cefc92dde9a9e276d7c710fdaa0f28663af1:base\n",
      "[13/22] Inserted up to 1 items for 0x57e104e68bbca6565486848349994d754ad61866:base\n",
      "[14/22] Inserted up to 1 items for 0xa3330ca4cd7ce3db9f6ca691b94f49c503dedca300ec6599cafa7e577ac946fc:aptos\n",
      "[15/22] Inserted up to 1 items for 0x5af0171ae867290357f2aef703a163f10f4b74b144534941457d3f5e217cf02c:aptos\n",
      "[16/22] Inserted up to 0 items for 0xa891b9211ce3b2a5bfc6f8bdbd837cda0641379a:ethereum\n",
      "[17/22] Inserted up to 1 items for 0xc219d80cac121b59c232041f72819d21e12e223ce8c543a7609a177b6fd16a0f:aptos\n",
      "[18/22] Inserted up to 10 items for 0x1195cf65f83b3a5768f3c496d3a05ad6412c64b7:etherlink\n",
      "[19/22] Inserted up to 10 items for 0x00b8f905c1ece49f78676733073e308fde4da737:base\n",
      "[20/22] Inserted up to 3 items for 0x93eca311f3a6f80df3a21a85fcfceeecf97f644b:polygon\n",
      "[21/22] Inserted up to 10 items for 0x23b5ba0f50664ce3a48ded214e773eb430818e65:celo\n",
      "[22/22] Inserted up to 5 items for 0x469f7afa52541afb334373368bf6d8dce891f219:base\n",
      "Done fetching and storing Rarible NFTs in rarible_nfts\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    RARIBLE_API_KEY = \"your_key\"\n",
    "    fetch_rarible_nfts_for_collections(\n",
    "        db_path=\"data/random_sample_2.db\",\n",
    "        table_collections=\"sampled_collections\",\n",
    "        table_nfts=\"rarible_nfts\",\n",
    "        api_key=RARIBLE_API_KEY,\n",
    "        size=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MagicEden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_magiceden_attributes(slug_name):\n",
    "    base_url = \"https://api-mainnet.magiceden.dev/v2/collections\"\n",
    "    url = f\"{base_url}/{slug_name}/attributes\"\n",
    "    headers = {\"accept\": \"application/json\"}\n",
    "    \n",
    "    resp = requests.get(url, headers=headers)\n",
    "    if resp.status_code == 200:\n",
    "        return resp.json()\n",
    "    else:\n",
    "        print(f\"[MagicEden Attributes] Error {resp.status_code} for slug='{slug_name}': {resp.text}\")\n",
    "        return {}\n",
    "\n",
    "def store_traits_json_for_magiceden(db_path, table_name, slug_name, traits_data):\n",
    "    \"\"\"\n",
    "    Updates the 'sampled_collections' row for marketplace='MagicEden'\n",
    "    and the given slug_name, setting the entire JSON in 'traits_json'.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    update_sql = f\"\"\"\n",
    "    UPDATE {table_name}\n",
    "    SET traits_json = ?\n",
    "    WHERE marketplace='MagicEden'\n",
    "      AND slug_name=?\n",
    "    \"\"\"\n",
    "    c.execute(update_sql, (json.dumps(traits_data), slug_name))\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def fetch_and_update_magiceden_traits(db_path=\"data/random_sample_2.db\", table_name=\"sampled_collections\"):\n",
    "    \"\"\"\n",
    "    1) Query 'sampled_collections' for rows with marketplace='MagicEden'.\n",
    "    2) For each 'slug_name', call fetch_magiceden_attributes(slug_name).\n",
    "    3) Store the JSON result in 'traits_json' column of the same row.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    # We'll fetch all MagicEden slug_names\n",
    "    c.execute(f\"\"\"\n",
    "    SELECT slug_name\n",
    "    FROM {table_name}\n",
    "    WHERE marketplace='MagicEden'\n",
    "      AND slug_name IS NOT NULL\n",
    "    \"\"\")\n",
    "    rows = c.fetchall()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"Found {len(rows)} Magic Eden collections to update 'traits_json'.\")\n",
    "    \n",
    "    count = 0\n",
    "    for (slug_name,) in rows:\n",
    "        data = fetch_magiceden_attributes(slug_name)\n",
    "        store_traits_json_for_magiceden(db_path, table_name, slug_name, data)\n",
    "        count += 1\n",
    "        print(f\"[{count}/{len(rows)}] Updated 'traits_json' for slug='{slug_name}'\")\n",
    "    \n",
    "    print(\"Done fetching/storing Magic Eden attributes in 'traits_json'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 Magic Eden collections to update 'traits_json'.\n",
      "[1/5] Updated 'traits_json' for slug='bit_monkey_club'\n",
      "[2/5] Updated 'traits_json' for slug='chorkean_'\n",
      "[3/5] Updated 'traits_json' for slug='monkelines'\n",
      "[4/5] Updated 'traits_json' for slug='p2_farmers_gs'\n",
      "[5/5] Updated 'traits_json' for slug='SAW_Games_Pass'\n",
      "Done fetching/storing Magic Eden attributes in 'traits_json'.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    fetch_and_update_magiceden_traits(\n",
    "        db_path=\"data/random_sample_2.db\",\n",
    "        table_name=\"sampled_collections\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_magiceden_nfts_table(db_path=\"data/random_sample_2.db\", table_name=\"magiceden_nfts\"):\n",
    "    \"\"\"\n",
    "    Creates a table 'magiceden_nfts' with columns for relevant fields from\n",
    "    the Magic Eden listing JSON, plus a link back to 'sampled_collections' via slug_name.\n",
    "    \n",
    "    We'll have 27 columns total:\n",
    "      1) id (PK)\n",
    "      2) collection_slug\n",
    "      ...\n",
    "      26) token_properties_json\n",
    "      27) fetched_at\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    # Below we define 27 columns total\n",
    "    c.execute(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        \n",
    "        collection_slug TEXT,\n",
    "\n",
    "        pdaAddress TEXT,\n",
    "        auctionHouse TEXT,\n",
    "        tokenAddress TEXT,\n",
    "        tokenMint TEXT,\n",
    "        seller TEXT,\n",
    "        sellerReferral TEXT,\n",
    "        tokenSize REAL,\n",
    "        price REAL,\n",
    "        expiry REAL,\n",
    "        \n",
    "        rarity_json TEXT,\n",
    "        extra_json TEXT,\n",
    "        listingSource TEXT,\n",
    "        \n",
    "        token_mintAddress TEXT,\n",
    "        token_owner TEXT,\n",
    "        token_supply REAL,\n",
    "        token_collection TEXT,\n",
    "        token_name TEXT,\n",
    "        token_updateAuthority TEXT,\n",
    "        token_primarySaleHappened INTEGER,\n",
    "        token_sellerFeeBasisPoints REAL,\n",
    "        token_image TEXT,\n",
    "        token_animationUrl TEXT,\n",
    "        token_externalUrl TEXT,\n",
    "        \n",
    "        token_attributes_json TEXT,\n",
    "        token_properties_json TEXT,\n",
    "        \n",
    "        fetched_at TEXT\n",
    "    )\n",
    "    \"\"\")\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def fetch_magiceden_listings(collection_slug, limit=10):\n",
    "    \"\"\"\n",
    "    GET https://api-mainnet.magiceden.dev/v2/collections/{collection_slug}/listings?limit={limit}\n",
    "    Returns a list of dicts if successful, else empty list.\n",
    "    \"\"\"\n",
    "    base_url = \"https://api-mainnet.magiceden.dev/v2/collections\"\n",
    "    url = f\"{base_url}/{collection_slug}/listings\"\n",
    "    params = {\"limit\": limit}\n",
    "    headers = {\"accept\": \"application/json\"}\n",
    "    \n",
    "    resp = requests.get(url, headers=headers, params=params)\n",
    "    if resp.status_code == 200:\n",
    "        return resp.json()  # Should be a list of listing dicts\n",
    "    else:\n",
    "        print(f\"[MagicEden NFT Fetch] Error {resp.status_code} for slug='{collection_slug}': {resp.text}\")\n",
    "        return []\n",
    "\n",
    "def store_magiceden_nfts(db_path, table_name, slug_name, listings):\n",
    "    \"\"\"\n",
    "    Insert each listing item into 'magiceden_nfts'.\n",
    "    We must provide EXACTLY 26 placeholders for these columns (excluding 'id' + including 'fetched_at' as a datetime call).\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    # 27 columns total => 'id' is autoincrement, so we have 26 placeholders\n",
    "    insert_sql = f\"\"\"\n",
    "    INSERT INTO {table_name} (\n",
    "        collection_slug,\n",
    "        \n",
    "        pdaAddress,\n",
    "        auctionHouse,\n",
    "        tokenAddress,\n",
    "        tokenMint,\n",
    "        seller,\n",
    "        sellerReferral,\n",
    "        tokenSize,\n",
    "        price,\n",
    "        expiry,\n",
    "        \n",
    "        rarity_json,\n",
    "        extra_json,\n",
    "        listingSource,\n",
    "        \n",
    "        token_mintAddress,\n",
    "        token_owner,\n",
    "        token_supply,\n",
    "        token_collection,\n",
    "        token_name,\n",
    "        token_updateAuthority,\n",
    "        token_primarySaleHappened,\n",
    "        token_sellerFeeBasisPoints,\n",
    "        token_image,\n",
    "        token_animationUrl,\n",
    "        token_externalUrl,\n",
    "        \n",
    "        token_attributes_json,\n",
    "        token_properties_json,\n",
    "        \n",
    "        fetched_at\n",
    "    )\n",
    "    VALUES (\n",
    "        ?,  -- collection_slug\n",
    "        ?,  -- pdaAddress\n",
    "        ?,  -- auctionHouse\n",
    "        ?,  -- tokenAddress\n",
    "        ?,  -- tokenMint\n",
    "        ?,  -- seller\n",
    "        ?,  -- sellerReferral\n",
    "        ?,  -- tokenSize\n",
    "        ?,  -- price\n",
    "        ?,  -- expiry\n",
    "        ?,  -- rarity_json\n",
    "        ?,  -- extra_json\n",
    "        ?,  -- listingSource\n",
    "        ?,  -- token_mintAddress\n",
    "        ?,  -- token_owner\n",
    "        ?,  -- token_supply\n",
    "        ?,  -- token_collection\n",
    "        ?,  -- token_name\n",
    "        ?,  -- token_updateAuthority\n",
    "        ?,  -- token_primarySaleHappened\n",
    "        ?,  -- token_sellerFeeBasisPoints\n",
    "        ?,  -- token_image\n",
    "        ?,  -- token_animationUrl\n",
    "        ?,  -- token_externalUrl\n",
    "        ?,  -- token_attributes_json\n",
    "        ?,  -- token_properties_json\n",
    "        datetime('now')  -- fetched_at\n",
    "    )\n",
    "    \"\"\"\n",
    "    # Notice we have exactly 26 placeholders (the last column uses datetime('now')).\n",
    "\n",
    "    for listing in listings:\n",
    "        pdaAddress = listing.get(\"pdaAddress\", \"\")\n",
    "        auctionHouse = listing.get(\"auctionHouse\", \"\")\n",
    "        tokenAddress = listing.get(\"tokenAddress\", \"\")\n",
    "        tokenMint = listing.get(\"tokenMint\", \"\")\n",
    "        seller = listing.get(\"seller\", \"\")\n",
    "        sellerReferral = listing.get(\"sellerReferral\", \"\")\n",
    "        tokenSize = listing.get(\"tokenSize\", 0)\n",
    "        price = listing.get(\"price\", 0)\n",
    "        expiry = listing.get(\"expiry\", 0)\n",
    "        \n",
    "        # Convert 'rarity' and 'extra' to JSON strings\n",
    "        rarity_json = json.dumps(listing.get(\"rarity\", {}))\n",
    "        extra_json = json.dumps(listing.get(\"extra\", {}))\n",
    "        \n",
    "        listingSource = listing.get(\"listingSource\", \"\")\n",
    "        \n",
    "        token_obj = listing.get(\"token\", {})\n",
    "        token_mintAddress = token_obj.get(\"mintAddress\", \"\")\n",
    "        token_owner = token_obj.get(\"owner\", \"\")\n",
    "        token_supply = token_obj.get(\"supply\", 0)\n",
    "        token_collection = token_obj.get(\"collection\", \"\")\n",
    "        token_name = token_obj.get(\"name\", \"\")\n",
    "        token_updateAuthority = token_obj.get(\"updateAuthority\", \"\")\n",
    "        \n",
    "        primarySaleHappened = 1 if token_obj.get(\"primarySaleHappened\", False) else 0\n",
    "        sellerFeeBasisPoints = token_obj.get(\"sellerFeeBasisPoints\", 0)\n",
    "        token_image = token_obj.get(\"image\", \"\")\n",
    "        token_animationUrl = token_obj.get(\"animationUrl\", \"\")\n",
    "        token_externalUrl = token_obj.get(\"externalUrl\", \"\")\n",
    "        \n",
    "        attributes_json = json.dumps(token_obj.get(\"attributes\", []))\n",
    "        properties_json = json.dumps(token_obj.get(\"properties\", {}))\n",
    "        \n",
    "        c.execute(insert_sql, (\n",
    "            slug_name,\n",
    "            pdaAddress,\n",
    "            auctionHouse,\n",
    "            tokenAddress,\n",
    "            tokenMint,\n",
    "            seller,\n",
    "            sellerReferral,\n",
    "            tokenSize,\n",
    "            price,\n",
    "            expiry,\n",
    "            rarity_json,\n",
    "            extra_json,\n",
    "            listingSource,\n",
    "            token_mintAddress,\n",
    "            token_owner,\n",
    "            token_supply,\n",
    "            token_collection,\n",
    "            token_name,\n",
    "            token_updateAuthority,\n",
    "            primarySaleHappened,\n",
    "            sellerFeeBasisPoints,\n",
    "            token_image,\n",
    "            token_animationUrl,\n",
    "            token_externalUrl,\n",
    "            attributes_json,\n",
    "            properties_json\n",
    "            # fetched_at => datetime('now') in SQL\n",
    "        ))\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def fetch_magiceden_nfts_for_collections(db_path=\"data/random_sample_2.db\",\n",
    "                                         table_collections=\"sampled_collections\",\n",
    "                                         table_nfts=\"magiceden_nfts\",\n",
    "                                         limit=10):\n",
    "    \"\"\"\n",
    "    Creates table if necessary, then for each row with marketplace='MagicEden',\n",
    "    uses 'slug_name' to fetch up to {limit} items from Magic Eden,\n",
    "    storing them in magiceden_nfts.\n",
    "    \"\"\"\n",
    "    create_magiceden_nfts_table(db_path, table_nfts)\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    c.execute(f\"\"\"\n",
    "    SELECT slug_name\n",
    "    FROM {table_collections}\n",
    "    WHERE marketplace='MagicEden'\n",
    "      AND slug_name IS NOT NULL\n",
    "    \"\"\")\n",
    "    rows = c.fetchall()\n",
    "    conn.close()\n",
    "    \n",
    "    count = 0\n",
    "    for (slug_name,) in rows:\n",
    "        data = fetch_magiceden_listings(slug_name, limit=limit)\n",
    "        store_magiceden_nfts(db_path, table_nfts, slug_name, data)\n",
    "        count += 1\n",
    "        print(f\"[{count}/{len(rows)}] Inserted up to {len(data)} items for MagicEden => {slug_name}\")\n",
    "    \n",
    "    print(f\"Done fetching/storing MagicEden listings in {table_nfts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/5] Inserted up to 10 items for MagicEden => bit_monkey_club\n",
      "[2/5] Inserted up to 2 items for MagicEden => chorkean_\n",
      "[3/5] Inserted up to 10 items for MagicEden => monkelines\n",
      "[4/5] Inserted up to 10 items for MagicEden => p2_farmers_gs\n",
      "[5/5] Inserted up to 10 items for MagicEden => SAW_Games_Pass\n",
      "Done fetching/storing MagicEden listings in magiceden_nfts\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    fetch_magiceden_nfts_for_collections(\n",
    "        db_path=\"data/random_sample_2.db\",\n",
    "        table_collections=\"sampled_collections\",\n",
    "        table_nfts=\"magiceden_nfts\",\n",
    "        limit=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atomic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_atomic_collection_stats(collection_name):\n",
    "    \"\"\"\n",
    "    Calls /atomicassets/v1/collections/{collection_name}/stats\"\n",
    "    \"\"\"\n",
    "    url = f\"https://wax.api.atomicassets.io/atomicassets/v1/collections/{collection_name}/stats\"\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code == 200:\n",
    "        return resp.json()\n",
    "    else:\n",
    "        print(f\"[Atomic Stats] Error {resp.status_code} for collection='{collection_name}': {resp.text}\")\n",
    "        return {}\n",
    "\n",
    "def update_atomic_collection_total_supply(db_path=\"data/random_sample_2.db\",\n",
    "                                          table_name=\"sampled_collections\"):\n",
    "    \"\"\"\n",
    "    1) Query 'sampled_collections' where marketplace='Atomic',\n",
    "       using 'slug_name' as the collection_name.\n",
    "    2) For each row, call fetch_atomic_collection_stats(...),\n",
    "       parse data[\"data\"][\"assets\"], and store it in 'total_supply'.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    c.execute(f\"\"\"\n",
    "    SELECT slug_name\n",
    "    FROM {table_name}\n",
    "    WHERE marketplace='Atomic'\n",
    "      AND slug_name IS NOT NULL\n",
    "    \"\"\")\n",
    "    rows = c.fetchall()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"Found {len(rows)} 'Atomic' collections to update total_supply.\")\n",
    "    \n",
    "    count = 0\n",
    "    for (collection_name,) in rows:\n",
    "        stats_data = fetch_atomic_collection_stats(collection_name)\n",
    "        # stats_data => { \"success\": bool, \"data\": {...}, ... }\n",
    "        if not stats_data.get(\"success\", False):\n",
    "            print(f\"Skipping {collection_name}, 'success' is False or missing.\")\n",
    "            continue\n",
    "        \n",
    "        # If \"assets\" is None or missing, default to \"0\"\n",
    "        assets_str = stats_data.get(\"data\", {}).get(\"assets\")\n",
    "        if assets_str is None:\n",
    "            assets_str = \"0\"\n",
    "        \n",
    "        try:\n",
    "            assets_count = int(assets_str)\n",
    "        except ValueError:\n",
    "            assets_count = 0  # or handle differently if needed\n",
    "        \n",
    "        conn2 = sqlite3.connect(db_path)\n",
    "        c2 = conn2.cursor()\n",
    "        update_stmt = f\"\"\"\n",
    "        UPDATE {table_name}\n",
    "        SET total_supply = ?\n",
    "        WHERE marketplace='Atomic'\n",
    "          AND slug_name=?\n",
    "        \"\"\"\n",
    "        c2.execute(update_stmt, (assets_count, collection_name))\n",
    "        conn2.commit()\n",
    "        conn2.close()\n",
    "        \n",
    "        count += 1\n",
    "        print(f\"[{count}/{len(rows)}] Updated total_supply={assets_count} for {collection_name}\")\n",
    "    \n",
    "    print(\"Done updating total_supply for Atomic collections.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 'Atomic' collections to update total_supply.\n",
      "[1/2] Updated total_supply=5 for bestpepeever\n",
      "[2/2] Updated total_supply=2 for vikez1modern\n",
      "Done updating total_supply for Atomic collections.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    update_atomic_collection_total_supply(\n",
    "        db_path=\"data/random_sample_2.db\",\n",
    "        table_name=\"sampled_collections\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_atomic_nfts_table(db_path=\"data/random_sample_2.db\", table_name=\"atomic_nfts\"):\n",
    "    \"\"\"\n",
    "    Creates a table that stores item-level data from the AtomicAssets API.\n",
    "    Adjust columns as needed to match your chosen fields.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    c.execute(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        \n",
    "        -- references the original 'collection_name' from 'sampled_collections' (Atomic)\n",
    "        collection_name TEXT,\n",
    "        \n",
    "        asset_id TEXT,\n",
    "        owner TEXT,\n",
    "        item_name TEXT,\n",
    "        is_transferable INTEGER,\n",
    "        is_burnable INTEGER,\n",
    "        \n",
    "        template_id TEXT,        -- from template[\"template_id\"]\n",
    "        max_supply TEXT,         -- from template[\"max_supply\"]\n",
    "        issued_supply TEXT,      -- from template[\"issued_supply\"]\n",
    "        \n",
    "        minted_at_time TEXT,     -- item[\"minted_at_time\"]\n",
    "        minted_at_block TEXT,    -- item[\"minted_at_block\"]\n",
    "        \n",
    "        data_json TEXT,          -- optionally store the entire item if you like\n",
    "        fetched_at TEXT\n",
    "    )\n",
    "    \"\"\")\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_atomic_assets(collection_name, limit=10):\n",
    "    \"\"\"\n",
    "    Calls the endpoint:\n",
    "      GET /atomicassets/v1/assets?collection_name={collection_name}&limit={limit}\n",
    "    Returns a dict with \"success\", \"data\" (list), and \"query_time\".\n",
    "    We'll return the entire JSON for reference.\n",
    "    \"\"\"\n",
    "    base_url = \"https://wax.api.atomicassets.io/atomicassets/v1/assets\"\n",
    "    params = {\n",
    "        \"collection_name\": collection_name,\n",
    "        \"limit\": limit\n",
    "    }\n",
    "    resp = requests.get(base_url, params=params)\n",
    "    if resp.status_code == 200:\n",
    "        return resp.json()  # e.g. { \"success\": true, \"data\": [...], \"query_time\": 0 }\n",
    "    else:\n",
    "        print(f\"[Atomic NFT Fetch] Error {resp.status_code} => {resp.text}\")\n",
    "        return {}\n",
    "\n",
    "def store_atomic_nfts(db_path, table_name, coll_name, items):\n",
    "    \"\"\"\n",
    "    Inserts the provided item-level data into 'atomic_nfts'.\n",
    "    items is a list of dicts from data[\"data\"] in the AtomicAssets response.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    insert_sql = f\"\"\"\n",
    "    INSERT INTO {table_name} (\n",
    "        collection_name,\n",
    "        asset_id,\n",
    "        owner,\n",
    "        item_name,\n",
    "        is_transferable,\n",
    "        is_burnable,\n",
    "        template_id,\n",
    "        max_supply,\n",
    "        issued_supply,\n",
    "        minted_at_time,\n",
    "        minted_at_block,\n",
    "        data_json,\n",
    "        fetched_at\n",
    "    )\n",
    "    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, datetime('now'))\n",
    "    \"\"\"\n",
    "    \n",
    "    for item in items:\n",
    "        asset_id = item.get(\"asset_id\", \"\")\n",
    "        owner = item.get(\"owner\", \"\")\n",
    "        item_name = item.get(\"name\", \"\")\n",
    "        \n",
    "        # Convert booleans to int\n",
    "        is_transferable = 1 if item.get(\"is_transferable\", False) else 0\n",
    "        is_burnable = 1 if item.get(\"is_burnable\", False) else 0\n",
    "        \n",
    "        # Safely handle template, which can be None\n",
    "        template = item.get(\"template\") or {}\n",
    "        template_id = template.get(\"template_id\", \"\")\n",
    "        max_supply = template.get(\"max_supply\", \"\")\n",
    "        issued_supply = template.get(\"issued_supply\", \"\")\n",
    "        \n",
    "        minted_at_time = item.get(\"minted_at_time\", \"\")\n",
    "        minted_at_block = item.get(\"minted_at_block\", \"\")\n",
    "        \n",
    "        data_json = json.dumps(item)\n",
    "        \n",
    "        c.execute(insert_sql, (\n",
    "            coll_name,\n",
    "            asset_id,\n",
    "            owner,\n",
    "            item_name,\n",
    "            is_transferable,\n",
    "            is_burnable,\n",
    "            template_id,\n",
    "            max_supply,\n",
    "            issued_supply,\n",
    "            minted_at_time,\n",
    "            minted_at_block,\n",
    "            data_json\n",
    "        ))\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "def fetch_atomic_nfts_for_collections(db_path=\"data/random_sample_2.db\",\n",
    "                                      table_collections=\"sampled_collections\",\n",
    "                                      table_nfts=\"atomic_nfts\",\n",
    "                                      limit=10):\n",
    "    \"\"\"\n",
    "    1) Create the 'atomic_nfts' table if not exists.\n",
    "    2) Query 'sampled_collections' for marketplace='Atomic'.\n",
    "       We'll assume there's a column 'collection_name' for each row.\n",
    "    3) For each row, call fetch_atomic_assets(collection_name, limit={limit}),\n",
    "       then store them in 'atomic_nfts'.\n",
    "    \"\"\"\n",
    "    # 1) create table\n",
    "    create_atomic_nfts_table(db_path, table_nfts)\n",
    "    \n",
    "    # 2) get the relevant rows from 'sampled_collections'\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    c.execute(f\"\"\"\n",
    "    SELECT slug_name\n",
    "    FROM {table_collections}\n",
    "    WHERE marketplace='Atomic'\n",
    "      AND slug_name IS NOT NULL\n",
    "    \"\"\")\n",
    "    rows = c.fetchall()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"Found {len(rows)} Atomic collections to fetch up to {limit} assets each.\")\n",
    "    \n",
    "    count = 0\n",
    "    for (coll_name,) in rows:\n",
    "        print(f\"[{count+1}/{len(rows)}] Fetching {limit} assets for collection_name='{coll_name}'\")\n",
    "        \n",
    "        data = fetch_atomic_assets(coll_name, limit=limit)\n",
    "        items = data.get(\"data\", [])  # a list of item dicts\n",
    "        \n",
    "        store_atomic_nfts(db_path, table_nfts, coll_name, items)\n",
    "        count += 1\n",
    "    \n",
    "    print(\"Done fetching/storing Atomic NFTs in\", table_nfts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 Atomic collections to fetch up to 10 assets each.\n",
      "[1/2] Fetching 10 assets for collection_name='bestpepeever'\n",
      "[2/2] Fetching 10 assets for collection_name='vikez1modern'\n",
      "Done fetching/storing Atomic NFTs in atomic_nfts\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    fetch_atomic_nfts_for_collections(\n",
    "        db_path=\"data/random_sample_2.db\",\n",
    "        table_collections=\"sampled_collections\",\n",
    "        table_nfts=\"atomic_nfts\",\n",
    "        limit=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
